{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a: Extracting Language Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Partic# Condition  gender  race\n",
      "428    837.0        AI     2.0     3\n",
      "429    838.0        AI     1.0     1\n",
      "430    839.0        AI     1.0     1\n",
      "431    840.0        AI     2.0     3\n",
      "432    841.0        AI     1.0     4\n",
      "   Participant_ID  PHQ_Score\n",
      "0             300          2\n",
      "1             301          3\n",
      "2             302          4\n",
      "3             303          0\n",
      "4             304          6\n"
     ]
    }
   ],
   "source": [
    "# the interview data (sheet 1)\n",
    "interview_data = pd.read_excel(\n",
    "    \"../data/DAIC_demographic_data.xlsx\",\n",
    "    sheet_name=\"Interview_Data\",\n",
    "    skiprows=lambda x: x == 1,\n",
    ")\n",
    "# drop the rows where data in column Partic# is NaN\n",
    "interview_data = interview_data.dropna(subset=[\"Partic#\"])\n",
    "print(interview_data.tail())\n",
    "\n",
    "# the phq score data (sheet 2)\n",
    "phq_score = pd.read_excel(\n",
    "    \"../data/DAIC_demographic_data.xlsx\", sheet_name=\"Metadata_mapping\"\n",
    ")\n",
    "\n",
    "print(phq_score.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step was a workaround to prevent the end of file error due to unclosed inverted commas. What the code is doing - opening and reading each file, reading each line and checking for lines that start with \" but does not end with \" (i.e., checking for unclosed quotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the csv files to remove the unclosed inverted commas\n",
    "def clean_csv(file_path, output_path):\n",
    "    with open(file_path, \"r\") as infile, open(output_path, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            if line.startswith('\"') and not line.endswith('\"\\n'):\n",
    "                line = line[1:]\n",
    "            outfile.write(line)\n",
    "\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            clean_csv(input_path, output_path)\n",
    "\n",
    "\n",
    "input_dir = \"../data/E-DAIC_Transcripts\"\n",
    "output_dir = \"../data/E-DAIC_Transcripts_cleaned\"\n",
    "\n",
    "process_directory(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>might have pulled something that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going to bring the great thanks so much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>are you okay with this yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh I'm fine I'm a little tired but I found ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>yeah well after college people usually many p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>thank you goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>oh that was that was fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>but I didn't never said there wasn't any like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>never know I guess</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text\n",
       "0                    might have pulled something that\n",
       "1         I'm going to bring the great thanks so much\n",
       "2                                          and please\n",
       "3                          are you okay with this yes\n",
       "4    oh I'm fine I'm a little tired but I found ou...\n",
       "..                                                ...\n",
       "76   yeah well after college people usually many p...\n",
       "77                                  thank you goodbye\n",
       "78                          oh that was that was fast\n",
       "79   but I didn't never said there wasn't any like...\n",
       "80                                 never know I guess\n",
       "\n",
       "[81 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_transcripts_path = \"../data/E-DAIC_Transcripts_cleaned\"\n",
    "\n",
    "transcripts = {}\n",
    "\n",
    "# loop through each file in the folder, load it, and store the content\n",
    "for filename in os.listdir(cleaned_transcripts_path):\n",
    "    if filename.endswith(\"_Transcript.csv\"):\n",
    "        participant_id = filename.split(\"_\")[0]  # extract the participant id\n",
    "        file_path = os.path.join(cleaned_transcripts_path, filename)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        transcripts[participant_id] = df\n",
    "\n",
    "# transcripts\n",
    "transcripts[\"386\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386 might have pulled something that I'm going to bring the great thanks so much and please are you okay with this yes oh I'm fine I'm a little tired but I found out my thyroid is I think acting up so where are you from originally I was born in Canada but I've lived in California most of my life so and it's gray today too so the gray weather makes you kind of see no sluggish oh my gosh years and years ago not at all I don't think I can text to her for a long time well the air fares have gone up quite a bit in the last few years gas prices have gone up so travel as much as it's a fun thing to do it's cost-prohibitive so oh that's see everything has plus and minuses cold-weather not as much sun so I think I'd prefer getting more sun and having the warm-weather so but other than that it's it's very pretty back their natural scenery in but I like it here you know the traffic is kind of heavy but that's a small thing to deal with why did I move to LA because my family came here and started to my dad got a job here and thus became our home base and that's why easy very easy sew well in high school you generally study General Ed they have designated courses and then after that went to college and I was a business major let's say I was a Veterinary major and I graduated with a degree in Liberal Arts so I don't know can't really pinpoint I was working in Social Services but I've also worked in sales and most recently I'm doing real estate I mean everything has plus and minuses it's it's okay it's not I don't really see too many negatives oh probably to work as a gossip columnist it TMZ or one of those networks but as you know everything in the entertainment business is very competitive it's who you know so but that would be my dream job or probably working with cats you know animals so I'm very outgoing in fact I think I'm so echoing I scare people sometimes there's a lot of quiet you know Reserve people out there so I'm kind of East Coast very you know I go up to people and talk to them first social sometimes when I'm feeling 10 I turn on the fish tank screensaver oh my gosh there's I could do hundreds of things I could read I could watch a show turn on the music music is great take a walk call my friends million things so I'm pretty good I'm very good I mean you know of course when you're driving in LA and somebody cuts in front of you sometimes I can be irritating but I'm I'm actually pretty I'm not the type of person that will fighter jet I don't like to have confrontations so I can't really remember so that's why I said there's not very many arguments usually I avoid arguments at any cost so do you feel guilty about I don't really feel guilty about anything right now or that I can think of I can't think of anything right now I really can't think of any decisions that were really hard I mean most of it's pretty easy for make me to make a decision so I don't have a family so if I have to you know I had to moves to San Francisco for a job once but since I'm single it's I don't really have to consider other people and children and that type of thing so kind of makes certain things easier when you don't have other family to consider yeah in advance well obviously you don't you know the passing of a family member you know you wish everybody could be around forever so I know when my father died you know I mean I wish I would have had him for a lot longer and he died at the age of 77 and you know what have been nice to have him around a little bit more but you know I can't control those things life has its own way of dealing with that yes there are Yin and Yang that's ancient philosophy and it really applies here is pluses and minuses the negatives and positives so well like I said my dad's no longer here and my mother and I you know we have our differences I think mothers and daughters always do my brother and I get along great I see him a lot my sister works a lot I see her and you know we talked and she's kind of got her own life and her children I get along great with them so we have a small family once again it's like that's a hard question to answer there's so many types of kids up there I think majority of them seem to be pretty well-behaved I mean you know there's always going to be children that are cut out of control but I don't know the ones that I've seen recently the very well behaved so can't say much else about that well I live in an apartment and I don't know if you've ever lived in apartment but there's someone that lives above me I'm on the first floor and there's somebody on the second floor and they're pretty quiet but they have a habit of I guess they have lots of energy they usually will take their showers around midnight and sometimes I'll start washing laundry at midnight and of course the pipes run through the bedroom wall so it sounds like Niagara Falls and they are and so I don't ya and I need 8 hours of sleep and I've only been getting oh I'd say like maybe 6 hours and then I usually wake up once at night take a sometimes they'll kind of around 2 or 3 in the morning go either walk around or I'll hear something so I'm not getting a straight 8 hours of sleep and it's definitely but there's not like I said there's things you can't control when you live in an apartment you know I means that the ladies look very quiet it just that she has a habit of you know taking later showers question close at night that's I can't say anything to her about that yeah but you know there's people that are in worse situations so I don't make a lot out of it you know and I just it's not like every night but it's you know at least twice a week so you know I have to consider sleeping in another room but for now you know that's I'll deal with it so tired very tired and hard to focus hard to concentrate it just takes me longer to get things done cuz I don't have my energy also I haven't been exercising I normally try to walk about half an hour a day but the weather's been kind of colder and kind of dreary or so you know I'm not as motivated when the weather is kind of blah so it's a combination of both not getting my exercise not getting 8 hours sleep how you been feeling lately as I said before tire has a combination of the weather you know I'm here in Los Angeles right now we have been having very gray days I live very close to the ocean so the sun doesn't come out that much so I'm not getting my serotonin I need that I am I getting my 8 hours of sleep because you know whatever and I think my thyroid is acting up too so that's kind of creating you know I have some vitamin deficiencies and you know once the Sun comes out I'm kind of more energetic it just gives you that more energy have you ever been diagnosed with p no no I hope I'm happy most of the time I'm a pretty happy person and I like to be around other people I mean of course I like my private time but I do like being around people quite a bit so can't think of anything right now I mean I usually enjoy everything I do there's nothing that stands out more than anything else okay is there anything you regret know I never regret anything what advice would I have given well I guess yeah I could probably change things around I think it everybody could I probably would have had a different college major in a seems like what else I probably wouldn't have dated certain waste of time with certain people I don't know those are them things that come to mind maybe started saving money earlier you know those types of things but how your best friend in Spanish outgoing lots of fun to be with just a good friend to have people like having me for a friend I'm loyal trusting and I listen I'm a good listener I'm very patient as well so what are some things you wish you could change about yourself well I never wanted me to lose weight but I think that's a worldwide problem I mean there's a lot of food around and temptations and especially here in America a lot of us are couch potatoes we sit in front of our computers we you know it's just easy to real it's so easy to relax you know it's hard to get up and move around you have to talk yourself into it so that's the way that you know house gives you more energy to if you lose weight so that's a big thing as well but that's probably the thing just the weight thing you I understand how do your best friend describe you I think you might have asked me that question before her but if you want I'll repeat it so I'm very like I said I'm very loyal when I tell people I'm going to do something I do it very reliable I am warm I'm very friendly great listener and I mean I've heard so many people tell me they just like being with me so like I said I'm very I'm pretty upbeat you know I'm don't let things get me down too much so how have in your life that's a hard question that can't think I mean I'm can't think of the answer that way so 4fun well I love movies so I could see every movie that they make I love going out with my friends shopping swap meets my gosh and traveling is great road trips that's kind of we know part of it yeah that I mentioned so traveling getting to see different places the history aspect of it the foods and joined Foods in different countries the architecture it's just kind of need to get away and see a completely different place okay well I've been to a lot of places in Europe I love Paris is such a walking City you can whoever designed that City I'll tell you I mean it's you can get from one point to another the Subways are great each area has its own distinctive kind of there's the Latin quarter where the students are there is another area where let's say the fashion designers are kind of like our Rodeo Drive here in Beverly Hills there's an area that is more African and they have swap meets in the weekends that are wonderful is for international and then like I said when she get out of Paris there's just so much history you can go see Versailles there's other palaces you can go to the south of France you know but that's probably my favorite country I've been to Italy as well so yeah well after college people usually many people that go to college or either before or after college that's a great time to go to Europe because you're young and you're not tied down you don't have a family and you kind of have a more adventuresome Spirit you got more energy so carrying you know backpack or suitcases it's not not a big deal so yeah that's thank you goodbye oh that was that was fast but I didn't never said there wasn't any like shocking question never know I guess\n",
      "387 when she's done she'll let you know alrighty thank you creative are you okay sure great Philadelphia the five years ago how often do you go back to your hometown once a year it doesn't can you tell me if La is a lot more fun for acting are you still doing that sure when I was young it was all that I wanted to do what's your dream job a dream job is an actor on an HBO series I'm sure you can tell by my shoes I'm not much of a world Explorer do you travel a lot moderately meeting new people eating new food I don't really feel like talking about it okay you can see yourself more shy I can be shy at times I can be outgoing maybe a little bit of both sometimes when I'm feeling I turn on the fish tank screensaver hey I know it's not Hawaii but it's the best I've got what do you do to relax just take a deep breath or maybe do some reading pretty good why was baby about 2 months ago my brother and I were arguing over rent payments yeah it does tell me about your relationship with your family very good very strong can't really say I don't really feel guilty often tell me about an event or something that you wish you could erase from your memory I really have anything that I'd like to erase from my memory okay what makes me mad ignorance racism things like that but I wish I would have handled differently I don't really feel like discussing that well I'm kind of in the midst of it right now I don't know whether I want to move to New York or to stay in La so it's a it's a tough decision to do what why don't we talk about that later okay how easy is it for you pretty easy what are you like when you don't sleep well I am sensitive and vulnerable sensitive and vulnerable as of lately I've been feeling pretty well take and play very clear minded thoughts no no last time I fell really really happy I had a real nice date but a month ago and we had a good time and I was real happy that day and the day after tell me about oh well I was a young girl that I had met a couple years ago and we finally or available to go out and hang out where you went and had some wine and a wine bar and then we went ahead dinner and I was the date had a great time thank you my ideal weekend well I like to watch English league soccer and sometimes I if I'm in Philly I spend time with my family on weekends if not I'll just hang out with my brother or Sundays are ref soccer just really laid-back and chill thank you no 10 or 20 years ago what advice would I give myself I would say never give up study hard save money maybe wish I was a little more outgoing little more enthusiastic in Social settings tell me about what made you feel really I really can't remember a time I I've always felt really good peoples whatever people have to say about me really doesn't change how I feel about myself he would say that very warm very caring loving genuine friend how my father yeah he is a so that he's a man of few words and he's very calm slow to anger and I'm very warm very loving man responsible and a gentleman is a great sense of style and he's a great cook what are you most proud of in your life most proud of 2 series regular on The Wire couple years ago in the last season thank you you welcome yeah okay\n",
      "388 are you okay with yes doing all right from Passaic New Jersey yes about 5 years ago try to go back every year Islam more people change of scenery I'm still trying to get used to it things you really low the people just dumb employment I was doing right now it's just no work I don't really have much of an education haven't I haven't figured it out yet I'm sure you can tell them do you travel a lot no not really I've got to be more shy now I'm not sure I'm just slime or Reserve now I'm not sure I just bill walls I guess I turn on screensaver hey I know it's not the right count to 10 I'm all right arguing all the time Jesus doing the dishes in the dish having two different things I don't want to do that can't wait to go on right now why don't we talk about that later tell me about Nevada or something that you wish you could something I wish I could erase my memory nothing really comes to mind right now join a blank I feel guilty about not having money for the bills not having a job not doing nothing with my life I am getting old not really tell me about the hardest decision you've ever had to make what kind of car to buy when was the last time that happened about 2 years ago can you tell me about that had the lottery search looking at magazines and just going on the internet just looking through cars and just seen you or what I wanted to get and what was in my budget inn how easy is it for you to get a good night sleep that's all right what are you like when you don't sleep well aggravated angry okay I'm feeling alright spin low down I'm sorry to hear that have you noticed any changes yeah well my eating Pat and I got to I'm just I don't have any like that but I just Restless in Moody have you ever been diagnosed with no no when I play with my dog this morning I just super bored or dad just need some time. For myself I just play with the dog yeah can I be as busy as possible wish I had an education in them better job time to do those things advice would you give yourself time go to school and stay in school go to school to learn I guess so I don't know Coopers in Vegas I like to joke and just have fun like to have a good time things don't go my way that that would be racing what forgot if I want to do something and I just can't do it that pisses me off no matter what it is it's just it's a problem not that I do whatever I want but that hinders. Positive my sister yeah well my sister she's a good person okay what are you most proud of in your life God is still be alive everything I need how is way too weird I don't know\n"
     ]
    }
   ],
   "source": [
    "# combine all the text data for each participant into a single string\n",
    "combined_transcripts = {}\n",
    "\n",
    "for participant_id, transcript in transcripts.items():\n",
    "    combined_transcripts[participant_id] = \" \".join(transcript[\"Text\"].astype(str))\n",
    "\n",
    "# removing extra spaces caused by newlines\n",
    "for participant_id, transcript in combined_transcripts.items():\n",
    "    combined_transcripts[participant_id] = \" \".join(transcript.split())\n",
    "\n",
    "# combined_transcripts\n",
    "\n",
    "for participant_id, transcript in list(combined_transcripts.items())[:3]:\n",
    "    print(participant_id, transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the demographic data with the extracted text data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# create the feature extractor objects here; using the base settings for now\n",
    "\n",
    "corpus = list(combined_transcripts.values())\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", token_pattern=r\"\\b[A-Za-z]+\\b\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "# convert the participant id in demographic data to int for consistency\n",
    "interview_data[\"Partic#\"] = interview_data[\"Partic#\"].astype(int)\n",
    "\n",
    "\n",
    "for _, row in interview_data.iterrows():\n",
    "    participant_id = str(row[\"Partic#\"])  # convert id to match the transcript ids\n",
    "\n",
    "    # find langauge features for this participant\n",
    "    if participant_id in combined_transcripts:\n",
    "        transcript = combined_transcripts[participant_id]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_pos_scores = sentiment_analyzer.polarity_scores(transcript)[\"pos\"]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_neu_scores = sentiment_analyzer.polarity_scores(transcript)[\"neu\"]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_neg_scores = sentiment_analyzer.polarity_scores(transcript)[\"neg\"]\n",
    "\n",
    "        # combine all the features\n",
    "        data = {\n",
    "            \"participant_id\": participant_id,\n",
    "            \"condition\": row[\"Condition\"],\n",
    "            \"race\": row[\"race\"],\n",
    "            \"genderData\": row[\"gender\"],\n",
    "            # \"tfidf_features\": tfidf_features,\n",
    "            # \"count_features\": count_features,\n",
    "            \"pos_sentiment\": sentiment_pos_scores,\n",
    "            \"neu_sentiment\": sentiment_neu_scores,\n",
    "            \"neg_sentiment\": sentiment_neg_scores,\n",
    "        }\n",
    "\n",
    "        combined_data.append(data)\n",
    "\n",
    "# convert the combined data into a dataframe\n",
    "combined_data_df = pd.DataFrame(combined_data)\n",
    "\n",
    "# print(combined_data_df.head())\n",
    "\n",
    "# create dataframes for tf-idf and count features\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# concatenate the original DataFrame with the TF-IDF and Count DataFrames\n",
    "final_df = pd.concat([combined_data_df.reset_index(drop=True), tfidf_df], axis=1)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By removing common stop words and number, we went from $8702$ features to $8299$ features. We need to investigate how this affects the accuracy.\n",
    "- Each term in the `TF-IDF` vector is considered a feature. The values represent the `TF-IDF` score for that term. A higher score could mean that the term is important to the transcript but not common in the entire list of transcripts. This is helpful in identifying the transcript's topic/sentiment.\n",
    "- Initially, I decided on using the compound score, but that resulting list of scores are heavily skewed. Figuring something might be wrong, I reverted back to using individual metrics (positive, negative and neutral)\n",
    "\n",
    "I noticed that using `CountVectorizer` just adds $8299$ more columns to the dataframe, making the already sparse dataset more sparse. Therefore, I decided to not use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature      score\n",
      "6326       s  35.261808\n",
      "7293       t  32.736003\n",
      "4428       m  29.000342\n",
      "4287    like  24.292047\n",
      "4058    just  23.032052\n",
      "4129    know  22.186805\n",
      "2193     don  17.789652\n",
      "5934  really  16.630834\n",
      "3160    good  12.510597\n",
      "7451   think  10.710480\n"
     ]
    }
   ],
   "source": [
    "# looking at the top 10 words with the highest tf-idf scores\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum the TF-IDF scores for each feature across all documents\n",
    "tfidf_scores = np.sum(tfidf_matrix.toarray(), axis=0)\n",
    "\n",
    "# Create a DataFrame with feature names and their corresponding scores\n",
    "tfidf_scores_df = pd.DataFrame({\"feature\": feature_names, \"score\": tfidf_scores})\n",
    "\n",
    "# Sort the DataFrame by scores in descending order\n",
    "tfidf_scores_df = tfidf_scores_df.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# Get the top 10 features\n",
    "top_10_features = tfidf_scores_df.head(10)\n",
    "\n",
    "# Display the top 10 features\n",
    "print(top_10_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed stop words number: 272\n"
     ]
    }
   ],
   "source": [
    "corpus = list(combined_transcripts.values())\n",
    "\n",
    "# create a TfidfVectorizer without removing stop words\n",
    "vectorizer_no_stop_words = TfidfVectorizer(token_pattern=r\"\\b[A-Za-z]+\\b\")\n",
    "vectorizer_no_stop_words.fit(corpus)\n",
    "vocab_no_stop_words = set(vectorizer_no_stop_words.get_feature_names_out())\n",
    "\n",
    "# create a TfidfVectorizer with stop_words='english'\n",
    "vectorizer_with_stop_words = TfidfVectorizer(\n",
    "    stop_words=\"english\", token_pattern=r\"\\b[A-Za-z]+\\b\"\n",
    ")\n",
    "vectorizer_with_stop_words.fit(corpus)\n",
    "vocab_with_stop_words = set(vectorizer_with_stop_words.get_feature_names_out())\n",
    "\n",
    "# find the difference between the two vocabularies\n",
    "removed_stop_words = vocab_no_stop_words - vocab_with_stop_words\n",
    "\n",
    "print(\"Removed stop words number:\", len(removed_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # creating 5 folds with shuffled data\\n\\nfrom sklearn.model_selection import KFold\\n\\n# initialize the KFold object with 5 splits\\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\\n\\nfolds = []\\n\\n# split the DataFrame into 5 folds\\nfor train_index, test_index in kf.split(final_df):\\n    train_fold = final_df.iloc[train_index]\\n    test_fold = final_df.iloc[test_index]\\n    folds.append((train_fold, test_fold))\\n\\n# display the first fold\\n# train_fold, test_fold = folds[0]\\n# print(\"Train Fold:\")\\n# print(train_fold.head())\\n# print(\"\\nTest Fold:\")\\n# print(test_fold.head())\\n\\nfor i, (train_fold, test_fold) in enumerate(folds):\\n    print(f\"Fold {i+1}:\")\\n    print(f\"Train Fold Size: {len(train_fold)}\")\\n    print(f\"Test Fold Size: {len(test_fold)}\\n\") '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # creating 5 folds with shuffled data\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# initialize the KFold object with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "folds = []\n",
    "\n",
    "# split the DataFrame into 5 folds\n",
    "for train_index, test_index in kf.split(final_df):\n",
    "    train_fold = final_df.iloc[train_index]\n",
    "    test_fold = final_df.iloc[test_index]\n",
    "    folds.append((train_fold, test_fold))\n",
    "\n",
    "# display the first fold\n",
    "# train_fold, test_fold = folds[0]\n",
    "# print(\"Train Fold:\")\n",
    "# print(train_fold.head())\n",
    "# print(\"\\nTest Fold:\")\n",
    "# print(test_fold.head())\n",
    "\n",
    "for i, (train_fold, test_fold) in enumerate(folds):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"Train Fold Size: {len(train_fold)}\")\n",
    "    print(f\"Test Fold Size: {len(test_fold)}\\n\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Doing more preprocessing to remove non-English words\n",
    "\n",
    "_I used the NLTK library to exclude non-English words from the corpus. This reduces the number of features significantly, as you'll see below. I'm still not sure about removing the stop words though; you can try and see if disabling it helps with the performance._\n",
    "\n",
    "_You can use this dataset to do the rest of the tasks. I only included this because I figured it might help with overfitting and consequentially, improve the model accuracies._\n",
    "\n",
    "_For the folds, I have used the `final_df` dataset, which is the one that has $8306$ features. If you want, you can use the dataframe with the non-English words removed._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Noah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>race</th>\n",
       "      <th>genderData</th>\n",
       "      <th>pos_sentiment</th>\n",
       "      <th>neu_sentiment</th>\n",
       "      <th>neg_sentiment</th>\n",
       "      <th>aa</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>yule</th>\n",
       "      <th>zany</th>\n",
       "      <th>zero</th>\n",
       "      <th>zest</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipping</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>386</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>387</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>388</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>389</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>390</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5334 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id condition  race  genderData  pos_sentiment  neu_sentiment  \\\n",
       "0            386       WoZ     3         2.0          0.184          0.770   \n",
       "1            387       WoZ     1         1.0          0.285          0.665   \n",
       "2            388       WoZ     4         1.0          0.161          0.769   \n",
       "3            389       WoZ     1         1.0          0.116          0.827   \n",
       "4            390       WoZ     3         1.0          0.193          0.740   \n",
       "\n",
       "   neg_sentiment   aa  abandoned  ability  ...  youth  yule  zany  zero  zest  \\\n",
       "0          0.046  0.0        0.0      0.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "1          0.050  0.0        0.0      0.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "2          0.070  0.0        0.0      0.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "3          0.057  0.0        0.0      0.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "4          0.067  0.0        0.0      0.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   zip  zipping  zodiac  zombie  zone  \n",
       "0  0.0      0.0     0.0     0.0   0.0  \n",
       "1  0.0      0.0     0.0     0.0   0.0  \n",
       "2  0.0      0.0     0.0     0.0   0.0  \n",
       "3  0.0      0.0     0.0     0.0   0.0  \n",
       "4  0.0      0.0     0.0     0.0   0.0  \n",
       "\n",
       "[5 rows x 5334 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download(\"words\")\n",
    "\n",
    "eng_words = set(words.words())\n",
    "\n",
    "# preprocessing function\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text)  # Tokenize\n",
    "    valid_words = [\n",
    "        token for token in tokens if token.lower() in eng_words\n",
    "    ]  # Filter non-English words\n",
    "    return \" \".join(valid_words)\n",
    "\n",
    "\n",
    "# create the feature extractor objects here; using the base settings for now\n",
    "\n",
    "# corpus = list(combined_transcripts.values())\n",
    "processed_corpus = [preprocess_text(text) for text in corpus]\n",
    "tfidf_vectorizer_ = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix_ = tfidf_vectorizer_.fit_transform(processed_corpus)\n",
    "\n",
    "sentiment_analyzer_ = SentimentIntensityAnalyzer()\n",
    "\n",
    "combined_data_ = []\n",
    "\n",
    "# convert the participant id in demographic data to int for consistency\n",
    "interview_data[\"Partic#\"] = interview_data[\"Partic#\"].astype(int)\n",
    "\n",
    "\n",
    "for _, row in interview_data.iterrows():\n",
    "    participant_id = str(row[\"Partic#\"])  # convert id to match the transcript ids\n",
    "\n",
    "    # find langauge features for this participant\n",
    "    if participant_id in combined_transcripts:\n",
    "        transcript = combined_transcripts[participant_id]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_pos_scores = sentiment_analyzer.polarity_scores(transcript)[\"pos\"]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_neu_scores = sentiment_analyzer.polarity_scores(transcript)[\"neu\"]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_neg_scores = sentiment_analyzer.polarity_scores(transcript)[\"neg\"]\n",
    "\n",
    "        # combine all the features\n",
    "        data = {\n",
    "            \"participant_id\": participant_id,\n",
    "            \"condition\": row[\"Condition\"],\n",
    "            \"race\": row[\"race\"],\n",
    "            \"genderData\": row[\"gender\"],\n",
    "            # \"tfidf_features\": tfidf_features,\n",
    "            # \"count_features\": count_features,\n",
    "            \"pos_sentiment\": sentiment_pos_scores,\n",
    "            \"neu_sentiment\": sentiment_neu_scores,\n",
    "            \"neg_sentiment\": sentiment_neg_scores,\n",
    "        }\n",
    "\n",
    "        combined_data_.append(data)\n",
    "\n",
    "# convert the combined data into a dataframe\n",
    "combined_data_df_ = pd.DataFrame(combined_data_)\n",
    "\n",
    "# print(combined_data_df.head())\n",
    "\n",
    "# create dataframes for tf-idf and count features\n",
    "tfidf_df_ = pd.DataFrame(\n",
    "    tfidf_matrix_.toarray(), columns=tfidf_vectorizer_.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# concatenate the original DataFrame with the TF-IDF and Count DataFrames\n",
    "final_df_ = pd.concat([combined_data_df_.reset_index(drop=True), tfidf_df_], axis=1)\n",
    "\n",
    "final_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By excluding non-english words, along with removing stop words, I was able to reduce the number of features significantly - from $8299$ to $5327$. This may produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature      score\n",
      "2752    like  29.028222\n",
      "2612    just  27.465559\n",
      "2650    know  26.661350\n",
      "1405     don  21.508594\n",
      "3770  really  19.885133\n",
      "2048    good  14.995024\n",
      "4768   think  12.867584\n",
      "3400  people  12.553898\n",
      "4795    time  12.395769\n",
      "2044   going  11.075604\n"
     ]
    }
   ],
   "source": [
    "feature_names_ = tfidf_vectorizer_.get_feature_names_out()\n",
    "\n",
    "# Sum the TF-IDF scores for each feature across all documents\n",
    "tfidf_scores_ = np.sum(tfidf_matrix_.toarray(), axis=0)\n",
    "\n",
    "# Create a DataFrame with feature names and their corresponding scores\n",
    "tfidf_scores_df_ = pd.DataFrame({\"feature\": feature_names_, \"score\": tfidf_scores_})\n",
    "\n",
    "# Sort the DataFrame by scores in descending order\n",
    "tfidf_scores_df_ = tfidf_scores_df_.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# Get the top 10 features\n",
    "top_10_features_ = tfidf_scores_df_.head(10)\n",
    "\n",
    "# Display the top 10 features\n",
    "print(top_10_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b: Classifying by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useNonEnglishWordRemovalDatasetFlag = False\n",
    "\n",
    "if useNonEnglishWordRemovalDatasetFlag:\n",
    "    genderDF = final_df_.drop([\"race\", \"condition\", \"participant_id\"], axis=1)\n",
    "else:\n",
    "    genderDF = final_df.drop([\"race\", \"condition\", \"participant_id\"], axis=1)\n",
    "\n",
    "genderDF = genderDF.dropna()\n",
    "\n",
    "# map gender from [1,2] -> [0,1] (XGBoost needs the labels to be 0 or 1)\n",
    "genderDF[\"genderData\"] = genderDF[\"genderData\"].map({1: 0, 2: 1})\n",
    "\n",
    "targetGender = genderDF.pop(\"genderData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "genderCorr = []\n",
    "\n",
    "for i, (cols) in enumerate(genderDF.columns):\n",
    "\n",
    "    corr, p = pearsonr(genderDF[cols], targetGender)\n",
    "    genderCorr.append(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Train Fold Size: 150\n",
      "Test Fold Size: 38\n",
      "\n",
      "Fold 2:\n",
      "Train Fold Size: 150\n",
      "Test Fold Size: 38\n",
      "\n",
      "Fold 3:\n",
      "Train Fold Size: 150\n",
      "Test Fold Size: 38\n",
      "\n",
      "Fold 4:\n",
      "Train Fold Size: 151\n",
      "Test Fold Size: 37\n",
      "\n",
      "Fold 5:\n",
      "Train Fold Size: 151\n",
      "Test Fold Size: 37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# initialize the KFold object with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "genderFolds = []\n",
    "\n",
    "# split the DataFrame into 5 folds\n",
    "for train_index, test_index in kf.split(genderDF):\n",
    "    train_fold = genderDF.iloc[train_index]\n",
    "    test_fold = genderDF.iloc[test_index]\n",
    "    train_target_fold = targetGender.iloc[train_index]\n",
    "    test_target_fold = targetGender.iloc[test_index]\n",
    "    genderFolds.append((train_fold, train_target_fold, test_fold, test_target_fold))\n",
    "\n",
    "for i, (train_fold, train_target_fold, test_fold, test_target_fold) in enumerate(genderFolds):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"Train Fold Size: {len(train_fold)}\")\n",
    "    print(f\"Test Fold Size: {len(test_fold)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to calcualte accuracy and balanced accuracy\n",
    "def getAccAndBAcc(yPred, yTrue):\n",
    "\n",
    "    truePos = 0\n",
    "    trueNeg = 0\n",
    "    falsePos = 0\n",
    "    falseNeg = 0\n",
    "\n",
    "    for idx in range(len(yPred)):\n",
    "        \n",
    "        if yPred[idx] == 1:\n",
    "\n",
    "            if yTrue[idx] == 1:\n",
    "\n",
    "                truePos += 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                falseNeg += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            if yTrue[idx] == 1:\n",
    "\n",
    "                falsePos += 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                trueNeg += 1\n",
    "\n",
    "    if (trueNeg+falsePos) == 0 and (truePos+falseNeg) != 0:\n",
    "        balancedAccuracy = 0.5*truePos/(truePos+falseNeg)\n",
    "    elif (trueNeg+falsePos) != 0 and (truePos+falseNeg) == 0:\n",
    "        balancedAccuracy = 0.5*trueNeg/(trueNeg+falsePos)\n",
    "    else:\n",
    "        balancedAccuracy = 0.5*trueNeg/(trueNeg+falsePos) + 0.5*truePos/(truePos+falseNeg)\n",
    "    \n",
    "    accuracy = (truePos + trueNeg) / (truePos + trueNeg + falsePos + falseNeg)\n",
    "\n",
    "    return accuracy, balancedAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create deep learning model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def getDLModel(inputShape):\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(inputShape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics = [\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 269us/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "\n",
      "\n",
      "#######################################\n",
      "Values for 8299 top features:\n",
      "Tree accuracy: 0.6541963015647226\n",
      "Tree balanced accuracy: 0.6286279289037909\n",
      "DL accuracy: 0.6113798008534851\n",
      "DL balanced accuracy: 0.30568990042674254\n",
      "#######################################\n",
      "Values for 4000 top features:\n",
      "Tree accuracy: 0.6600284495021338\n",
      "Tree balanced accuracy: 0.6597334147334147\n",
      "DL accuracy: 0.6113798008534851\n",
      "DL balanced accuracy: 0.30568990042674254\n",
      "#######################################\n",
      "Values for 3000 top features:\n",
      "Tree accuracy: 0.6331436699857753\n",
      "Tree balanced accuracy: 0.6218678931582157\n",
      "DL accuracy: 0.6113798008534851\n",
      "DL balanced accuracy: 0.30568990042674254\n",
      "#######################################\n",
      "Values for 2000 top features:\n",
      "Tree accuracy: 0.7237553342816501\n",
      "Tree balanced accuracy: 0.7313808260704813\n",
      "DL accuracy: 0.6113798008534851\n",
      "DL balanced accuracy: 0.30568990042674254\n",
      "#######################################\n",
      "Values for 1000 top features:\n",
      "Tree accuracy: 0.7237553342816501\n",
      "Tree balanced accuracy: 0.7313808260704813\n",
      "DL accuracy: 0.6113798008534851\n",
      "DL balanced accuracy: 0.30568990042674254\n",
      "#######################################\n",
      "Values for 500 top features:\n",
      "Tree accuracy: 0.7446657183499289\n",
      "Tree balanced accuracy: 0.7501807464566086\n",
      "DL accuracy: 0.6275960170697014\n",
      "DL balanced accuracy: 0.41117479708810983\n",
      "\n",
      "\n",
      "#######################################\n",
      "Max Tree Number of Features: 500, with balanced accuracy of 75.01807464566086%\n",
      "Max DL Number of Features: 500, with balanced accuracy of 41.11747970881098%\n",
      "#######################################\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "numberOfFeatures = [len(genderDF.columns)-1, 4000, 3000, 2000, 1000, 500]\n",
    "\n",
    "bAccTree = []\n",
    "accTree  = []\n",
    "\n",
    "bAccDL = []\n",
    "accDL  = []\n",
    "\n",
    "top_n_idx = lambda data, n: np.argsort(data)[-n:]\n",
    "\n",
    "for n in numberOfFeatures:\n",
    "\n",
    "    nIdx = top_n_idx(genderCorr, n)\n",
    "\n",
    "    nCols = genderDF.columns[nIdx]\n",
    "\n",
    "    foldAccTree = []\n",
    "    foldBAccTree = []\n",
    "\n",
    "    foldAccDL = []\n",
    "    foldBAccDL = []\n",
    "\n",
    "    for i, (train_fold, train_target_fold, test_fold, test_target_fold) in enumerate(genderFolds):\n",
    "\n",
    "        train_fold = train_fold[nCols]\n",
    "        test_fold = test_fold[nCols]\n",
    "\n",
    "        # train XGBoost model\n",
    "        xgb_classifier = xgb.XGBClassifier()\n",
    "        xgb_classifier.fit(train_fold, train_target_fold)\n",
    "\n",
    "        # get test accuracy and balanced accuracy\n",
    "        acc, bAcc = getAccAndBAcc(xgb_classifier.predict(test_fold), test_target_fold.to_numpy())\n",
    "\n",
    "        # store accuracies\n",
    "        foldAccTree.append(acc)\n",
    "        foldBAccTree.append(bAcc)\n",
    "\n",
    "        # train DL model\n",
    "        FNN = getDLModel(train_fold.shape[1])\n",
    "        FNN.fit(train_fold, train_target_fold, epochs=100, verbose=0)\n",
    "\n",
    "        # get test accuracy and balanced accuracy\n",
    "        acc, bAcc = getAccAndBAcc(FNN.predict(test_fold), test_target_fold.to_numpy())\n",
    "\n",
    "        # store accuracies\n",
    "        foldAccDL.append(acc)\n",
    "        foldBAccDL.append(bAcc)\n",
    "\n",
    "    accTree.append(np.mean(foldAccTree))\n",
    "    bAccTree.append(np.mean(foldBAccTree))\n",
    "    \n",
    "    accDL.append(np.mean(foldAccDL))\n",
    "    bAccDL.append(np.mean(foldBAccDL))\n",
    "\n",
    "print(\"\\n\")\n",
    "for idx in range(len(numberOfFeatures)):\n",
    "\n",
    "    print(\"#######################################\")\n",
    "    print(f\"Values for {numberOfFeatures[idx]} top features:\")\n",
    "    print(f\"Tree accuracy: {accTree[idx]}\")\n",
    "    print(f\"Tree balanced accuracy: {bAccTree[idx]}\")\n",
    "    print(f\"DL accuracy: {accDL[idx]}\")\n",
    "    print(f\"DL balanced accuracy: {bAccDL[idx]}\")\n",
    "\n",
    "\n",
    "maxTreeBAccNumFeatures = numberOfFeatures[np.argmax(bAccTree)]\n",
    "maxDLBAccNumFeatures = numberOfFeatures[np.argmax(bAccDL)]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"#######################################\")\n",
    "print(f\"Max Tree Number of Features: {maxTreeBAccNumFeatures}, with balanced accuracy of {np.max(bAccTree) * 100}%\")\n",
    "print(f\"Max DL Number of Features: {maxDLBAccNumFeatures}, with balanced accuracy of {np.max(bAccDL) * 100}%\")\n",
    "print(\"#######################################\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
