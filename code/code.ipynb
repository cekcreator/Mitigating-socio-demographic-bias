{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a: Extracting Language Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Partic# Condition  gender  race\n",
      "428    837.0        AI     2.0     3\n",
      "429    838.0        AI     1.0     1\n",
      "430    839.0        AI     1.0     1\n",
      "431    840.0        AI     2.0     3\n",
      "432    841.0        AI     1.0     4\n",
      "   Participant_ID  PHQ_Score\n",
      "0             300          2\n",
      "1             301          3\n",
      "2             302          4\n",
      "3             303          0\n",
      "4             304          6\n"
     ]
    }
   ],
   "source": [
    "# the interview data (sheet 1)\n",
    "interview_data = pd.read_excel(\n",
    "    \"../data/DAIC_demographic_data.xlsx\",\n",
    "    sheet_name=\"Interview_Data\",\n",
    "    skiprows=lambda x: x == 1,\n",
    ")\n",
    "# drop the rows where data in column Partic# is NaN\n",
    "interview_data = interview_data.dropna(subset=[\"Partic#\"])\n",
    "print(interview_data.tail())\n",
    "\n",
    "# the phq score data (sheet 2)\n",
    "phq_score = pd.read_excel(\n",
    "    \"../data/DAIC_demographic_data.xlsx\", sheet_name=\"Metadata_mapping\"\n",
    ")\n",
    "\n",
    "print(phq_score.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step was a workaround to prevent the end of file error due to unclosed inverted commas. What the code is doing - opening and reading each file, reading each line and checking for lines that start with \" but does not end with \" (i.e., checking for unclosed quotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the csv files to remove the unclosed inverted commas\n",
    "def clean_csv(file_path, output_path):\n",
    "    with open(file_path, \"r\") as infile, open(output_path, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            if line.startswith('\"') and not line.endswith('\"\\n'):\n",
    "                line = line[1:]\n",
    "            outfile.write(line)\n",
    "\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            clean_csv(input_path, output_path)\n",
    "\n",
    "\n",
    "input_dir = \"../data/E-DAIC_Transcripts\"\n",
    "output_dir = \"../data/E-DAIC_Transcripts_cleaned\"\n",
    "\n",
    "process_directory(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>might have pulled something that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going to bring the great thanks so much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>are you okay with this yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh I'm fine I'm a little tired but I found ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>yeah well after college people usually many p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>thank you goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>oh that was that was fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>but I didn't never said there wasn't any like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>never know I guess</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text\n",
       "0                    might have pulled something that\n",
       "1         I'm going to bring the great thanks so much\n",
       "2                                          and please\n",
       "3                          are you okay with this yes\n",
       "4    oh I'm fine I'm a little tired but I found ou...\n",
       "..                                                ...\n",
       "76   yeah well after college people usually many p...\n",
       "77                                  thank you goodbye\n",
       "78                          oh that was that was fast\n",
       "79   but I didn't never said there wasn't any like...\n",
       "80                                 never know I guess\n",
       "\n",
       "[81 rows x 1 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_transcripts_path = \"../data/E-DAIC_Transcripts_cleaned\"\n",
    "\n",
    "transcripts = {}\n",
    "\n",
    "# loop through each file in the folder, load it, and store the content\n",
    "for filename in os.listdir(cleaned_transcripts_path):\n",
    "    if filename.endswith(\"_Transcript.csv\"):\n",
    "        participant_id = filename.split(\"_\")[0]  # extract the participant id\n",
    "        file_path = os.path.join(cleaned_transcripts_path, filename)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        transcripts[participant_id] = df\n",
    "\n",
    "# transcripts\n",
    "transcripts[\"386\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423 okay and please yes feeling well where are you from originally Los Angeles the Greater Los Angeles area people diversity and various entertainment and activities fickle weather traffic and litter do you travel I have travel domestically not internationally seeing other places and how people live in the culture I want to hear about one of your trips the last trip that I went to a San Diego butt out from childhood or as an adult family vacations to Baja in Rosarito Beach can you tell me yeah the reason why that memory comes to mind because this weekend is Easter and we usually will go down to East Easter spring break to Rosarito Baja am I have a Bachelors in communication studies because of the open field just doesn't deal with them communication interactively but Performing Arts telecommunications business entrepreneurship so no I'm not at Maxey unemployed what's your dream job I think 200 my business that's my dream my dream job to have my own company I would like to open something like the Telecommunications area as a business something do online online services I like to make things with my hands so my next venture with would be to create all-natural soy candles something in that area a mixture mixture of shy and outgoing because I have to observe people in crowds and that's how I usually make a determination if I remain reserved remain open I listen to like a meditation tape or I take a walk or go get a massage Fair oh man I probably yesterday or this morning usually the argument is about me making myself clear Army repeating myself twice no usually gets the other person in trouble tell me about a situation that you wish you would have I redeemed a service from a mechanic who fix my transmission and I had a warranty and I wrote to the Department of Consumer Affairs and the guy offered me a portion of the settlement agreement and I decided to take him to small claims court to get a bigger amount and I wish I would have have taking the offer that he gave me at the midst of dealing with a third party which was the Department of Consumer Affairs and because of that I took him to small claims court and I end up losing the whole settlement and only been offered a very small portion so I wish I would have taken the higher road to taking a lower Road I don't tend to feel guilty about too many things tell me about the hardest decision you've ever had to make hardest decision I don't know financially emotionally emotionally letting go of past relationship the hardest thing to say the hardest thing but it was a difficult thing can you tell me about yes it was a relationship of 7 years but that person is already moved on so they want me to be in their life and hopes that if something goes wrong with my current relationship will always have something to fall back on that sounds really hard oh yeah it is hard tell me about your bag or something that you wish you could erase from your in advance so that I can trace for my memory I don't know there's a lot of events that I just automatically erased from my memory but whatever comes to mind right probably the death of a relative I'm alone and misinformed because I didn't know that they were that sick and I didn't know that they were their days were numbered and I could have had more opportunity to spend time with them and make better a better connection in the last days I'm sorry to hear that tell me about your relationship with you is Fair to Midland are you into your family my immediate I'm very close but distant not very close okay my mother tell me about and yes I she's encouraged me to continue my education to always fall back on my hobbies hopes golden dreams she always encourages me to always stay loose and out there looking for a better opportunity and she encourages me to not be afraid to explore other Ventures how easy is it for you to get a good night sleep easy I've been feeling well yes a little stressed but I'm trying to manage that I leave the stressful environment in the past about 3 years ago because I was having issues in my learning environment and I was feeling emotionally attached yeah I was having an issue with a professor who was just very I know you have a very different personality he almost ran me down the crosswalk at school how do you feel I felt I felt like I was in a dishonest environment I felt like I wanted to get you know switch his class and get into another class but I had no choice but to stay in there and deal with his meanness for the whole semester have you been diagnosed with depression I think through finding PTSD they were trying to link depression you still go to therapy now no financially I'm not capable and I haven't found a counselor who's willing and committed to working with me and helping me with my issues I find a lot of counselors they listen to me and then they laugh at me and then they tell me the opposite of what's what I'm telling them last time I felt really happy I really don't know that's a good question something you did recently that you really enjoy just just do what I want I just relaxed I just relaxed and you know I don't know I don't have any best friends my best qualities that I'm hard-working I'm reliable I'm an excellent friend I'm energetic and I'm going to go get her things that I wish I could change about myself probably be less sensitive don't take things so personally and learn how to help myself before helping others no I don't have any regrets in my life if I could do things differently 10 years ago I probably would have focus and study more and less being out of the house and hanging with friends what are you most proud of in your life baguette have a college education and I'm self-sufficient and I can pay my bills by myself and I'm alive goodbye hi so then where you at that was exactly\n",
      "436 that was big yes I'm doing fine Mexico when I was young seven years song I've been there like that three or four times for vacation well it's a less stressful I don't know if it's because I'm on vacation but I feel more at peace over there than here well my family moved down here for a better life that was my parents just the weather and different things that you could do you could be at the beach or you could go to the mountains different variety of things to do the traffic and some basically the traffic and that's one of the main things nothing much I've been to New York and Mexico only I just seen you places and finding new locations you know something new like for a trip over I've been to Mexico I've been to the pyramids of I forgot to call in Mexico City and I like to know about finding a lot of my Asian culture since I'm Mexican from Mayan Aztecs I like to see different things like that well that's cool why did drop out but I came back and I'm trying to get back into it right now trying to learn graphic designs well I started liking you know to take pictures doing different things and I like you know going into business I like what the titles about I want to have my own business either making web pages just be able to be free and don't be in relying on someone else just be able to have freedom for myself how's it going Spectre you know be around people like to get to know people and depending on the person how I feel around them I'd like to hear music or you know it was a different thing that case boards or two different things just to unwind well I'm getting better at it you know Hyatt connect I was like 2 days ago I was something about that I needed to pay bills on something that I did not have taken care of but the person he's like more picky about it and I was more know how gross are you know that they want everything right there and then I have a handle you know I have everything there and they just bugging sometimes they don't give you the space that you need to erase all something I felt like I was overwhelmed already had it resolved I just was waiting for a check you know it's not on my control but I already had made the paper proper arrangements and she's bugging me about it it's like you're making an argument and storm in the glass of water the different need thirst couple of them but not so recently because I've been more trying to get my life together but since I made before in the past I dropping out of school and do another different thanks guilting not that much because not really not that I can think right up top of my head the hardest decision I was just on top of my head right now the band the only things that I remember is when I black out or different things like that that I remember nothing like a particular only with my brothers died and different things like that how my brother was in gangs and different things and he was getting more to trouble and it's just not a pleasant memory for me to talk about that much get into it don't understand it. You know I like to be with my family and you know I like to go over there to Mexico to I have a half-sister and my grandpa died he was older Mexico so I do I like to have a good relationship with my family my mom and dad they always work hard they always done what they have to do to pay the bills that's why sometimes I be like you no running around trying to get everything done but done correctly not trying to take the easy way out it's a little hard because they're especially here in Los Angeles there's a lot of different things that you can get into that's not that positive and it's hard to stay focused on you know the correct stuff Well normally I just knock out it's not that hard it's like to take me like half an hour or 15 minutes but sometimes I can even wake up even with the alarm they have to wake me up in know my grass wake me up in with the alarm going on it's like so hard Cena I just knock out I do get a little either I'm angry or I don't feel good and then I'll feel bad energy but I still have to know I know I have to do something so I still go through with it I've been swept in the feeling good do you know then that little restless but not that much I have to do stuff so I guess you know for myself to do it no matter how I feel nothing out of the ordinary no the last time iPhone happy that was following the weekend that I would see not going out with a cloud have some fun and they know I was drinking and just relaxing I want winding down ideal weekend is usually with my girl either out or at home just relax when nothing else you know just be able to relax and unwind with her you know you were going out somewhere tonight place we're staying at home that without getting any calls or in the interruption might just be able to turn off the phone and turn off everything just be what are doing the other different things like an outgoing guy in a very social but at the same time I'm reserved depending how I feel with people at the cross her feel comfortable with them well I'm changing a lot of my temper and different things drinking things on changing the different things I could get more different things done there's several but mostly is when I'm coming from Mexico that they wanted the meaner you even down there I feel like silly because I went to a store I was regularly dressed and I was the time I wanted to buy something and then they said that that I wasn't belong there and I ended soon as they saw my credit card they kind of change perspective because I saw the Wells Fargo logo they know it's American but they thought it was let you know what is told me it's all ridiculous to me because you go to a place you know you have the money ask for something you can ask for something because you have it and not just the way to kill the time just because they saw a logo of a card that's how that you know it was ridiculous for me no basically to stay more focused on different things that stay in school and then I don't go to not try to control my temper and do different things cuz if I knew what I know now back then he'll be different well I did drop out but I did go back to school and I know I did get my high school and right now I'm deposit to another different thing so right now is getting my high school getting back together right now I'm working to another girl I'm trying to get me know either going back to school so the basic basically getting back to school and finishing the high school for GED awkward like I thought it would\n",
      "447 yeah that's perfectly fine I'm feeling great it's an early morning I don't have school born and raised in Los Angeles California the fact that you can serve snowboard and like do to drastically different things in one day coming from the neighborhoods I've lived in I'd say the population of lower socio-economic backgrounds it's like a cesspool almost for young African-Americans and it sucks not really I've been to New York Connecticut Mexico and Texas but pretty young mostly if I'm traveling by plane just decide in the air is amazing it's remarkable the first time I went to New York I was about 11 years old and I made sure I got the the window seat on the plane and just a feeling of taking off is just exhilarating but we left at night here in LA and it was the next day in New York so it's kind of confusing getting used to the time change and the first thing I remember doing is getting a slice of New York Pizza it was it was delicious yep graduating with the bachelor's degree in Communications so I can go into the military as an officer what made you decide to do actually like would I don't know it's like you got to have a plan it's the only thing that's just falling into place and it seems like it's going to fall into place for me what's your dream job to be honest job that doesn't exist however you can get paid to do nothing yeah but probably I was definitely shy as a kid but I've grown into myself through the experience of college so I'm way more outspoken I write music and I work out mostly so if I'm feeling stressed out I just do a set of push-ups and lifting weights I'm pretty good I saw a therapist when I was a kid for about a year and I don't know if he told me to just be able to control my emotions and I don't think I do it in the best way because when I'm angry I do decide to like just bottled in my emotions instead of letting them out in their appropriate Outlets not really it's been more of a in interpersonal thing that's I don't know I don't know probably yesterday something in regards to sports the playoffs are on so things like that than Hu so many I guess in my senior year of high school I waited till the last minute to apply for the UC schools and it was unsuccessful because I didn't get to turn in applications and I'm pretty sure I would have been admitted so it sucks but hey tell me about that is an intriguing question I've actually no clue the death of my grandmother sure my grandmother died when I was 11 years old she raised me from the time I was born because my mother was unable to take care of me she raised my sister my brother and my cousin is well and her five children so she's the matriarch of our family and the closest thing I've had to a mother it was tough but it's going to be 7 years dismay next on the 30th and I've grown quite a bit No More Tears I'm a foster kid so I was emancipated from the Foster program about 2 years ago now but my immediate family are pretty close with him as much as I can and my foster family they're awesome I've known them since I was 5 so it's been pretty cool how easy is it for you to get a good night sleep fairly easy I fall asleep pretty quick it's not a problem except for when my back's hurting and then it's a problem a little moody in the morning I guess I drive more aggressively a little stressed about a few problems that I can take care of but I shouldn't stress because either can do something about it or I can't mess this way things work can you tell me about Usher I have a few parking ticket not parking tickets are not moving violations but their tickets and I was unaware of if you miss Court without being in the hospital or being in jail that you couldn't be excused from not been making your court date so I have 3 failure to appears in court with tickets that are thousands of dollars and I have no money so I don't know if it's pretty tough to deal with when you're trying to make something out of your life but there's so many barriers and obstacles that like you you didn't foresee so I don't know it gets stressful at times and yeah thinking about it for periods of times some days I just sit and I'm just lost in my thoughts and I don't really communicate to anyone because there's nothing anybody can do for me early so I just bottle it up like I do everything not very after doing it for so long it becomes a natural thing I believe no no really happy why is that such a difficult question I don't know I'm pretty content with everything right now I don't know about extreme happiness everyday is just the same day seems like dunking across the street at the Clippers center is a basketball court there and I've been working on it for a while and I finally got up yet I have 4 close friends that I've known for over eight years each and I don't know Studios hard-working determined I don't know yeah probably my metabolism which I don't really have control over some I don't know I'm pretty content with myself tell me about I think in my sixth grade year of grade school I was I wasn't the best of students but I had a teacher who was diligent and like he felt me the whole year but I learned a lot and I developed a passion for school afterwards so it was like a situation I overcome after feeling so bad and down the same teacher who made me feel really bad actually because it wasn't for his like sternness I don't think I would have gained the discipline to be where I am at now in college and stuff so I remember this time I was in the desert or up in Joshua Tree with my best friend and his dad and we actually got stuck in the mountains and like we went down a gully and his Jeep would get over a rock so it was it was pretty yes it was kind of weird and I thought I was Superman I try to move a rock that was thousands of pounds didn't work but yeah we eventually got out without going to be stuck in the desert but it was a fun time I got to drive in the desert when I didn't have my license owes fun about that day or in general other than like the UC thing like what I didn't apply to the schools and time I regret that I don't know I was I was 10 years old 10 years ago I probably say to myself no clue that's that's an interesting question Bing the first from my Mediacom immediate family is in my mother my brothers and my sister to go to college so yes it's pretty no problem\n"
     ]
    }
   ],
   "source": [
    "# combine all the text data for each participant into a single string\n",
    "combined_transcripts = {}\n",
    "\n",
    "for participant_id, transcript in transcripts.items():\n",
    "    combined_transcripts[participant_id] = \" \".join(transcript[\"Text\"].astype(str))\n",
    "\n",
    "# removing extra spaces caused by newlines\n",
    "for participant_id, transcript in combined_transcripts.items():\n",
    "    combined_transcripts[participant_id] = \" \".join(transcript.split())\n",
    "\n",
    "# combined_transcripts\n",
    "\n",
    "for participant_id, transcript in list(combined_transcripts.items())[:3]:\n",
    "    print(participant_id, transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>pos_sentiment</th>\n",
       "      <th>neu_sentiment</th>\n",
       "      <th>neg_sentiment</th>\n",
       "      <th>aa</th>\n",
       "      <th>aau</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>...</th>\n",
       "      <th>zip</th>\n",
       "      <th>ziplining</th>\n",
       "      <th>zipping</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zoloft</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>386</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>387</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>388</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>389</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>390</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id condition  race  gender  pos_sentiment  neu_sentiment  \\\n",
       "0            386       WoZ     3     2.0          0.184          0.770   \n",
       "1            387       WoZ     1     1.0          0.285          0.665   \n",
       "2            388       WoZ     4     1.0          0.161          0.769   \n",
       "3            389       WoZ     1     1.0          0.116          0.827   \n",
       "4            390       WoZ     3     1.0          0.193          0.740   \n",
       "\n",
       "   neg_sentiment   aa  aau  abandoned  ...  zip  ziplining  zipping  zodiac  \\\n",
       "0          0.046  0.0  0.0   0.000000  ...  0.0        0.0      0.0     0.0   \n",
       "1          0.050  0.0  0.0   0.000000  ...  0.0        0.0      0.0     0.0   \n",
       "2          0.070  0.0  0.0   0.000000  ...  0.0        0.0      0.0     0.0   \n",
       "3          0.057  0.0  0.0   0.000000  ...  0.0        0.0      0.0     0.0   \n",
       "4          0.067  0.0  0.0   0.055628  ...  0.0        0.0      0.0     0.0   \n",
       "\n",
       "   zoloft  zombie  zone  zuma  zurich   zz  \n",
       "0     0.0     0.0   0.0   0.0     0.0  0.0  \n",
       "1     0.0     0.0   0.0   0.0     0.0  0.0  \n",
       "2     0.0     0.0   0.0   0.0     0.0  0.0  \n",
       "3     0.0     0.0   0.0   0.0     0.0  0.0  \n",
       "4     0.0     0.0   0.0   0.0     0.0  0.0  \n",
       "\n",
       "[5 rows x 8306 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the demographic data with the extracted text data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# create the feature extractor objects here; using the base settings for now\n",
    "\n",
    "corpus = list(combined_transcripts.values())\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", token_pattern=r\"\\b[A-Za-z]+\\b\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "# convert the participant id in demographic data to int for consistency\n",
    "interview_data[\"Partic#\"] = interview_data[\"Partic#\"].astype(int)\n",
    "\n",
    "\n",
    "for _, row in interview_data.iterrows():\n",
    "    participant_id = str(row[\"Partic#\"])  # convert id to match the transcript ids\n",
    "\n",
    "    # find langauge features for this participant\n",
    "    if participant_id in combined_transcripts:\n",
    "        transcript = combined_transcripts[participant_id]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_pos_scores = sentiment_analyzer.polarity_scores(transcript)[\"pos\"]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_neu_scores = sentiment_analyzer.polarity_scores(transcript)[\"neu\"]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_neg_scores = sentiment_analyzer.polarity_scores(transcript)[\"neg\"]\n",
    "\n",
    "        # combine all the features\n",
    "        data = {\n",
    "            \"participant_id\": participant_id,\n",
    "            \"condition\": row[\"Condition\"],\n",
    "            \"race\": row[\"race\"],\n",
    "            \"gender\": row[\"gender\"],\n",
    "            # \"tfidf_features\": tfidf_features,\n",
    "            # \"count_features\": count_features,\n",
    "            \"pos_sentiment\": sentiment_pos_scores,\n",
    "            \"neu_sentiment\": sentiment_neu_scores,\n",
    "            \"neg_sentiment\": sentiment_neg_scores,\n",
    "        }\n",
    "\n",
    "        combined_data.append(data)\n",
    "\n",
    "# convert the combined data into a dataframe\n",
    "combined_data_df = pd.DataFrame(combined_data)\n",
    "\n",
    "# print(combined_data_df.head())\n",
    "\n",
    "# create dataframes for tf-idf and count features\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# concatenate the original DataFrame with the TF-IDF and Count DataFrames\n",
    "final_df = pd.concat([combined_data_df.reset_index(drop=True), tfidf_df], axis=1)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By removing common stop words and number, we went from $8702$ features to $8299$ features. We need to investigate how this affects the accuracy.\n",
    "- Each term in the `TF-IDF` vector is considered a feature. The values represent the `TF-IDF` score for that term. A higher score could mean that the term is important to the transcript but not common in the entire list of transcripts. This is helpful in identifying the transcript's topic/sentiment.\n",
    "- Initially, I decided on using the compound score, but that resulting list of scores are heavily skewed. Figuring something might be wrong, I reverted back to using individual metrics (positive, negative and neutral)\n",
    "\n",
    "I noticed that using `CountVectorizer` just adds $8299$ more columns to the dataframe, making the already sparse dataset more sparse. Therefore, I decided to not use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature      score\n",
      "6326       s  35.261808\n",
      "7293       t  32.736003\n",
      "4428       m  29.000342\n",
      "4287    like  24.292047\n",
      "4058    just  23.032052\n",
      "4129    know  22.186805\n",
      "2193     don  17.789652\n",
      "5934  really  16.630834\n",
      "3160    good  12.510597\n",
      "7451   think  10.710480\n"
     ]
    }
   ],
   "source": [
    "# looking at the top 10 words with the highest tf-idf scores\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum the TF-IDF scores for each feature across all documents\n",
    "tfidf_scores = np.sum(tfidf_matrix.toarray(), axis=0)\n",
    "\n",
    "# Create a DataFrame with feature names and their corresponding scores\n",
    "tfidf_scores_df = pd.DataFrame({\"feature\": feature_names, \"score\": tfidf_scores})\n",
    "\n",
    "# Sort the DataFrame by scores in descending order\n",
    "tfidf_scores_df = tfidf_scores_df.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# Get the top 10 features\n",
    "top_10_features = tfidf_scores_df.head(10)\n",
    "\n",
    "# Display the top 10 features\n",
    "print(top_10_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed stop words number: 272\n"
     ]
    }
   ],
   "source": [
    "corpus = list(combined_transcripts.values())\n",
    "\n",
    "# create a TfidfVectorizer without removing stop words\n",
    "vectorizer_no_stop_words = TfidfVectorizer(token_pattern=r\"\\b[A-Za-z]+\\b\")\n",
    "vectorizer_no_stop_words.fit(corpus)\n",
    "vocab_no_stop_words = set(vectorizer_no_stop_words.get_feature_names_out())\n",
    "\n",
    "# create a TfidfVectorizer with stop_words='english'\n",
    "vectorizer_with_stop_words = TfidfVectorizer(\n",
    "    stop_words=\"english\", token_pattern=r\"\\b[A-Za-z]+\\b\"\n",
    ")\n",
    "vectorizer_with_stop_words.fit(corpus)\n",
    "vocab_with_stop_words = set(vectorizer_with_stop_words.get_feature_names_out())\n",
    "\n",
    "# find the difference between the two vocabularies\n",
    "removed_stop_words = vocab_no_stop_words - vocab_with_stop_words\n",
    "\n",
    "print(\"Removed stop words number:\", len(removed_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Train Fold Size: 152\n",
      "Test Fold Size: 38\n",
      "\n",
      "Fold 2:\n",
      "Train Fold Size: 152\n",
      "Test Fold Size: 38\n",
      "\n",
      "Fold 3:\n",
      "Train Fold Size: 152\n",
      "Test Fold Size: 38\n",
      "\n",
      "Fold 4:\n",
      "Train Fold Size: 152\n",
      "Test Fold Size: 38\n",
      "\n",
      "Fold 5:\n",
      "Train Fold Size: 152\n",
      "Test Fold Size: 38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating 5 folds with shuffled data\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# initialize the KFold object with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "folds = []\n",
    "\n",
    "# split the DataFrame into 5 folds\n",
    "for train_index, test_index in kf.split(final_df):\n",
    "    train_fold = final_df.iloc[train_index]\n",
    "    test_fold = final_df.iloc[test_index]\n",
    "    folds.append((train_fold, test_fold))\n",
    "\n",
    "# display the first fold\n",
    "# train_fold, test_fold = folds[0]\n",
    "# print(\"Train Fold:\")\n",
    "# print(train_fold.head())\n",
    "# print(\"\\nTest Fold:\")\n",
    "# print(test_fold.head())\n",
    "\n",
    "for i, (train_fold, test_fold) in enumerate(folds):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"Train Fold Size: {len(train_fold)}\")\n",
    "    print(f\"Test Fold Size: {len(test_fold)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Doing more preprocessing to remove non-English words\n",
    "\n",
    "_I used the NLTK library to exclude non-English words from the corpus. This reduces the number of features significantly, as you'll see below. I'm still not sure about removing the stop words though; you can try and see if disabling it helps with the performance._\n",
    "\n",
    "_You can use this dataset to do the rest of the tasks. I only included this because I figured it might help with overfitting and consequentially, improve the model accuracies._\n",
    "\n",
    "_For the folds, I have used the `final_df` dataset, which is the one that has $8306$ features. If you want, you can use the dataframe with the non-English words removed._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/arindrajitpaul/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>pos_sentiment</th>\n",
       "      <th>neu_sentiment</th>\n",
       "      <th>neg_sentiment</th>\n",
       "      <th>aa</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>yule</th>\n",
       "      <th>zany</th>\n",
       "      <th>zero</th>\n",
       "      <th>zest</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipping</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>386</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>387</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>388</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>389</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>390</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5334 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id condition  race  gender  pos_sentiment  neu_sentiment  \\\n",
       "0            386       WoZ     3     2.0          0.184          0.770   \n",
       "1            387       WoZ     1     1.0          0.285          0.665   \n",
       "2            388       WoZ     4     1.0          0.161          0.769   \n",
       "3            389       WoZ     1     1.0          0.116          0.827   \n",
       "4            390       WoZ     3     1.0          0.193          0.740   \n",
       "\n",
       "   neg_sentiment   aa  abandoned  ability  ...  youth  yule  zany  zero  zest  \\\n",
       "0          0.046  0.0   0.000000      0.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "1          0.050  0.0   0.000000      0.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "2          0.070  0.0   0.000000      0.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "3          0.057  0.0   0.000000      0.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "4          0.067  0.0   0.068101      0.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   zip  zipping  zodiac  zombie  zone  \n",
       "0  0.0      0.0     0.0     0.0   0.0  \n",
       "1  0.0      0.0     0.0     0.0   0.0  \n",
       "2  0.0      0.0     0.0     0.0   0.0  \n",
       "3  0.0      0.0     0.0     0.0   0.0  \n",
       "4  0.0      0.0     0.0     0.0   0.0  \n",
       "\n",
       "[5 rows x 5334 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download(\"words\")\n",
    "\n",
    "eng_words = set(words.words())\n",
    "\n",
    "# preprocessing function\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text)  # Tokenize\n",
    "    valid_words = [\n",
    "        token for token in tokens if token.lower() in eng_words\n",
    "    ]  # Filter non-English words\n",
    "    return \" \".join(valid_words)\n",
    "\n",
    "\n",
    "# create the feature extractor objects here; using the base settings for now\n",
    "\n",
    "# corpus = list(combined_transcripts.values())\n",
    "processed_corpus = [preprocess_text(text) for text in corpus]\n",
    "tfidf_vectorizer_ = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix_ = tfidf_vectorizer_.fit_transform(processed_corpus)\n",
    "\n",
    "sentiment_analyzer_ = SentimentIntensityAnalyzer()\n",
    "\n",
    "combined_data_ = []\n",
    "\n",
    "# convert the participant id in demographic data to int for consistency\n",
    "interview_data[\"Partic#\"] = interview_data[\"Partic#\"].astype(int)\n",
    "\n",
    "\n",
    "for _, row in interview_data.iterrows():\n",
    "    participant_id = str(row[\"Partic#\"])  # convert id to match the transcript ids\n",
    "\n",
    "    # find langauge features for this participant\n",
    "    if participant_id in combined_transcripts:\n",
    "        transcript = combined_transcripts[participant_id]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_pos_scores = sentiment_analyzer.polarity_scores(transcript)[\"pos\"]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_neu_scores = sentiment_analyzer.polarity_scores(transcript)[\"neu\"]\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_neg_scores = sentiment_analyzer.polarity_scores(transcript)[\"neg\"]\n",
    "\n",
    "        # combine all the features\n",
    "        data = {\n",
    "            \"participant_id\": participant_id,\n",
    "            \"condition\": row[\"Condition\"],\n",
    "            \"race\": row[\"race\"],\n",
    "            \"gender\": row[\"gender\"],\n",
    "            # \"tfidf_features\": tfidf_features,\n",
    "            # \"count_features\": count_features,\n",
    "            \"pos_sentiment\": sentiment_pos_scores,\n",
    "            \"neu_sentiment\": sentiment_neu_scores,\n",
    "            \"neg_sentiment\": sentiment_neg_scores,\n",
    "        }\n",
    "\n",
    "        combined_data_.append(data)\n",
    "\n",
    "# convert the combined data into a dataframe\n",
    "combined_data_df_ = pd.DataFrame(combined_data_)\n",
    "\n",
    "# print(combined_data_df.head())\n",
    "\n",
    "# create dataframes for tf-idf and count features\n",
    "tfidf_df_ = pd.DataFrame(\n",
    "    tfidf_matrix_.toarray(), columns=tfidf_vectorizer_.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# concatenate the original DataFrame with the TF-IDF and Count DataFrames\n",
    "final_df_ = pd.concat([combined_data_df_.reset_index(drop=True), tfidf_df_], axis=1)\n",
    "\n",
    "final_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By excluding non-english words, along with removing stop words, I was able to reduce the number of features significantly - from $8299$ to $5327$. This may produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature      score\n",
      "2752    like  29.028222\n",
      "2612    just  27.465559\n",
      "2650    know  26.661350\n",
      "1405     don  21.508594\n",
      "3770  really  19.885133\n",
      "2048    good  14.995024\n",
      "4768   think  12.867584\n",
      "3400  people  12.553898\n",
      "4795    time  12.395769\n",
      "2044   going  11.075604\n"
     ]
    }
   ],
   "source": [
    "feature_names_ = tfidf_vectorizer_.get_feature_names_out()\n",
    "\n",
    "# Sum the TF-IDF scores for each feature across all documents\n",
    "tfidf_scores_ = np.sum(tfidf_matrix_.toarray(), axis=0)\n",
    "\n",
    "# Create a DataFrame with feature names and their corresponding scores\n",
    "tfidf_scores_df_ = pd.DataFrame({\"feature\": feature_names_, \"score\": tfidf_scores_})\n",
    "\n",
    "# Sort the DataFrame by scores in descending order\n",
    "tfidf_scores_df_ = tfidf_scores_df_.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# Get the top 10 features\n",
    "top_10_features_ = tfidf_scores_df_.head(10)\n",
    "\n",
    "# Display the top 10 features\n",
    "print(top_10_features_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
