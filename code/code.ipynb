{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:42:49.024804Z",
     "start_time": "2024-11-28T17:42:49.016145Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a: Extracting Language Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:42:49.726914Z",
     "start_time": "2024-11-28T17:42:49.666659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Partic# Condition  gender  race\n",
      "428    837.0        AI     2.0     3\n",
      "429    838.0        AI     1.0     1\n",
      "430    839.0        AI     1.0     1\n",
      "431    840.0        AI     2.0     3\n",
      "432    841.0        AI     1.0     4\n",
      "  Participant_ID  PHQ_Score\n",
      "0            300          2\n",
      "1            301          3\n",
      "2            302          4\n",
      "3            303          0\n",
      "4            304          6\n"
     ]
    }
   ],
   "source": [
    "# the interview data (sheet 1)\n",
    "interview_data = pd.read_excel(\n",
    "    \"../data/DAIC_demographic_data.xlsx\",\n",
    "    sheet_name=\"Interview_Data\",\n",
    "    skiprows=lambda x: x == 1,\n",
    ")\n",
    "# drop the rows where data in column Partic# is NaN\n",
    "interview_data = interview_data.dropna(subset=[\"Partic#\"])\n",
    "print(interview_data.tail())\n",
    "\n",
    "# the phq score data (sheet 2)\n",
    "phq_scores_data = pd.read_excel(\n",
    "    \"../data/DAIC_demographic_data.xlsx\", sheet_name=\"Metadata_mapping\"\n",
    ")\n",
    "\n",
    "#Convert Participant_ID column to string\n",
    "phq_scores_data['Participant_ID'] = phq_scores_data['Participant_ID'].astype(str)\n",
    "\n",
    "#Convert PHQ_Score to int\n",
    "phq_scores_data['PHQ_Score'] = phq_scores_data['PHQ_Score'].astype(int)\n",
    "\n",
    "print(phq_scores_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step was a workaround to prevent the end of file error due to unclosed inverted commas. What the code is doing - opening and reading each file, reading each line and checking for lines that start with \" but does not end with \" (i.e., checking for unclosed quotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:42:50.585238Z",
     "start_time": "2024-11-28T17:42:50.498474Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing the csv files to remove the unclosed inverted commas\n",
    "def clean_csv(file_path, output_path):\n",
    "    with open(file_path, \"r\") as infile, open(output_path, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            if line.startswith('\"') and not line.endswith('\"\\n'):\n",
    "                line = line[1:]\n",
    "            outfile.write(line)\n",
    "\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            clean_csv(input_path, output_path)\n",
    "\n",
    "\n",
    "input_dir = \"../data/E-DAIC_Transcripts\"\n",
    "output_dir = \"../data/E-DAIC_Transcripts_cleaned\"\n",
    "\n",
    "process_directory(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:42:51.290958Z",
     "start_time": "2024-11-28T17:42:51.208151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>might have pulled something that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going to bring the great thanks so much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>are you okay with this yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh I'm fine I'm a little tired but I found ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>yeah well after college people usually many p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>thank you goodbye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>oh that was that was fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>but I didn't never said there wasn't any like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>never know I guess</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text\n",
       "0                    might have pulled something that\n",
       "1         I'm going to bring the great thanks so much\n",
       "2                                          and please\n",
       "3                          are you okay with this yes\n",
       "4    oh I'm fine I'm a little tired but I found ou...\n",
       "..                                                ...\n",
       "76   yeah well after college people usually many p...\n",
       "77                                  thank you goodbye\n",
       "78                          oh that was that was fast\n",
       "79   but I didn't never said there wasn't any like...\n",
       "80                                 never know I guess\n",
       "\n",
       "[81 rows x 1 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_transcripts_path = \"../data/E-DAIC_Transcripts_cleaned\"\n",
    "\n",
    "transcripts = {}\n",
    "\n",
    "# loop through each file in the folder, load it, and store the content\n",
    "for filename in os.listdir(cleaned_transcripts_path):\n",
    "    if filename.endswith(\"_Transcript.csv\"):\n",
    "        participant_id = filename.split(\"_\")[0]  # extract the participant id\n",
    "        file_path = os.path.join(cleaned_transcripts_path, filename)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        transcripts[participant_id] = df\n",
    "\n",
    "# transcripts\n",
    "transcripts[\"386\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:42:51.993151Z",
     "start_time": "2024-11-28T17:42:51.955109Z"
    }
   },
   "outputs": [],
   "source": [
    "# combine all the text data for each participant into a single string\n",
    "combined_transcripts = {}\n",
    "\n",
    "for participant_id, transcript in transcripts.items():\n",
    "    combined_transcripts[participant_id] = \" \".join(transcript[\"Text\"].astype(str))\n",
    "\n",
    "# removing extra spaces caused by newlines\n",
    "for participant_id, transcript in combined_transcripts.items():\n",
    "    combined_transcripts[participant_id] = \" \".join(transcript.split())\n",
    "\n",
    "# combined_transcripts[\"386\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:43:02.268734Z",
     "start_time": "2024-11-28T17:42:52.638775Z"
    }
   },
   "outputs": [],
   "source": [
    "# combine the demographic data with the extracted text data\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# create the feature extractor objects here; using the base settings for now\n",
    "# TODO: make changes to the parameters to preprocess the text data\n",
    "\n",
    "corpus = list(combined_transcripts.values())\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(corpus)\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(corpus)\n",
    "\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "# convert the participant id in demographic data to int for consistency\n",
    "interview_data[\"Partic#\"] = interview_data[\"Partic#\"].astype(int)\n",
    "\n",
    "for _, row in interview_data.iterrows():\n",
    "    participant_id = str(row[\"Partic#\"])  # convert id to match the transcript ids\n",
    "\n",
    "    # find langauge features for this participant\n",
    "    if participant_id in combined_transcripts:\n",
    "        transcript = combined_transcripts[participant_id]\n",
    "\n",
    "        # tfidf features\n",
    "        tfidf_features = (\n",
    "            tfidf_vectorizer.transform([transcript]).toarray().flatten()\n",
    "        )  # has to be 1D array\n",
    "\n",
    "        # count features\n",
    "        count_features = count_vectorizer.transform([transcript]).toarray().flatten()\n",
    "\n",
    "        # sentiment features\n",
    "        sentiment_compound_scores = sentiment_analyzer.polarity_scores(transcript)[\n",
    "            \"compound\"\n",
    "        ]  # only extracting the compound score\n",
    "\n",
    "        # Extracting the PHQ score\n",
    "        phq_score_row = phq_scores_data[phq_scores_data[\"Participant_ID\"] == participant_id]\n",
    "        if not phq_score_row.empty:\n",
    "            phq_score = phq_score_row[\"PHQ_Score\"].values[0]\n",
    "        else:\n",
    "            phq_score = -1\n",
    "            continue  # skip this participant if PHQ score is not found\n",
    "\n",
    "        # combine all the features\n",
    "        data = {\n",
    "            \"participant_id\": participant_id,\n",
    "            \"condition\": row[\"Condition\"],\n",
    "            \"race\": row[\"race\"],\n",
    "            \"PHQ_score\": phq_score,\n",
    "            \"gender\": row[\"gender\"],\n",
    "            \"tfidf_features\": tfidf_features,\n",
    "            \"count_features\": count_features,\n",
    "            \"overall_sentiment\": sentiment_compound_scores\n",
    "        }\n",
    "\n",
    "        combined_data.append(data)\n",
    "\n",
    "# convert the combined data into a dataframe\n",
    "combined_data_df = pd.DataFrame(combined_data)\n",
    "\n",
    "# print(combined_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:43:02.303492Z",
     "start_time": "2024-11-28T17:43:02.285073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>condition</th>\n",
       "      <th>race</th>\n",
       "      <th>PHQ_score</th>\n",
       "      <th>gender</th>\n",
       "      <th>tfidf_features</th>\n",
       "      <th>count_features</th>\n",
       "      <th>overall_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>386</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0085443313426...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>387</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0253774451976...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>388</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0292778521617...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.9953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>389</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0487055979434...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>0.9822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>390</td>\n",
       "      <td>WoZ</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.03182282795453762, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.9996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id condition  race  PHQ_score  gender  \\\n",
       "0            386       WoZ     3         11     2.0   \n",
       "1            387       WoZ     1          2     1.0   \n",
       "2            388       WoZ     4         17     1.0   \n",
       "3            389       WoZ     1         14     1.0   \n",
       "4            390       WoZ     3          9     1.0   \n",
       "\n",
       "                                      tfidf_features  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0085443313426...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0253774451976...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0292778521617...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0487055979434...   \n",
       "4  [0.03182282795453762, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                      count_features  overall_sentiment  \n",
       "0  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...             0.9999  \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...             0.9996  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...             0.9953  \n",
       "3  [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, ...             0.9822  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             0.9996  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful information about the resulting dataframe - \n",
    "\n",
    "- Each term in the `TF-IDF` vector is considered a feature. The values represent the `TF-IDF` score for that term. A higher score could mean that the term is important to the transcript but no common in the entire list of transcripts. This is helpful in identifying the transcript's topic/sentiment.\n",
    "- Count features are straightforward. The value of a feature is the raw count of how many times the term appears in the transcript.\n",
    "- The compound score is the overall sentiment of the transcript. Its value ranges from $-1$ to $1$ where positive sentiments have a compound score of $\\geq 0.05$, neutral sentiment have a score between $-0.05$ and $0.05$, and negative sentiments have a compound score of $\\leq -0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b: Classifying for gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:12:20.341041Z",
     "start_time": "2024-11-28T17:12:20.330573Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop race, condition, phq_score, and participant_id\n",
    "genderDF = combined_data_df.drop([\"race\", \"condition\", \"participant_id\", \"PHQ_score\"], axis=1)\n",
    "\n",
    "# drop any row with NaN\n",
    "genderDF = genderDF.dropna()\n",
    "\n",
    "# map gender from [1,2] -> [0,1] (XGBoost needs the labels to be 0 or 1)\n",
    "genderDF[\"gender\"] = genderDF[\"gender\"].map({1: 0, 2: 1})\n",
    "\n",
    "# extract the target values (gender)\n",
    "y = genderDF.pop(\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:12:21.944427Z",
     "start_time": "2024-11-28T17:12:21.301582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 17404)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost cannot accept multi-dimensional features so each list element must live in its own column\n",
    "\n",
    "# Expand lists into separate columns\n",
    "tfidf_features = pd.DataFrame(genderDF['tfidf_features'].tolist(), index=genderDF.index)\n",
    "tfidf_features.columns = [f'tfidf_features{i}' for i in range(tfidf_features.shape[1])]\n",
    "\n",
    "# Expand lists into separate columns\n",
    "count_features = pd.DataFrame(genderDF['count_features'].tolist(), index=genderDF.index)\n",
    "count_features.columns = [f'count_features{i}' for i in range(count_features.shape[1])]\n",
    "\n",
    "# recreate gender dataframe\n",
    "genderDF = pd.concat([tfidf_features, count_features], axis=1)\n",
    "\n",
    "genderDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:12:23.969642Z",
     "start_time": "2024-11-28T17:12:23.211035Z"
    }
   },
   "outputs": [],
   "source": [
    "# due to the flattening of the arrays, we not how 17404 features which is too much\n",
    "# therefore, we are going to use PCA to reduce our input dimensionality\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale data before applying PCA\n",
    "scaling=StandardScaler()\n",
    " \n",
    "# Use fit and transform method \n",
    "scaling.fit(genderDF)\n",
    "Scaled_data=scaling.transform(genderDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:12:28.926527Z",
     "start_time": "2024-11-28T17:12:28.920583Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper function to calculate accuracy and balanced accuracy\n",
    "def getAccAndBAcc(yPred, yTrue):\n",
    "\n",
    "    truePos = 0\n",
    "    trueNeg = 0\n",
    "    falsePos = 0\n",
    "    falseNeg = 0\n",
    "\n",
    "    for idx in range(len(yPred)):\n",
    "        \n",
    "        if yPred[idx] == 1:\n",
    "\n",
    "            if yTrue[idx] == 1:\n",
    "\n",
    "                truePos += 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                falseNeg += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            if yTrue[idx] == 1:\n",
    "\n",
    "                falsePos += 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                trueNeg += 1\n",
    "\n",
    "    if (trueNeg+falsePos) == 0 and (truePos+falseNeg) != 0:\n",
    "        balancedAccuracy = 0.5*truePos/(truePos+falseNeg)\n",
    "    elif (trueNeg+falsePos) != 0 and (truePos+falseNeg) == 0:\n",
    "        balancedAccuracy = 0.5*trueNeg/(trueNeg+falsePos)\n",
    "    else:\n",
    "        balancedAccuracy = 0.5*trueNeg/(trueNeg+falsePos) + 0.5*truePos/(truePos+falseNeg)\n",
    "    \n",
    "    accuracy = (truePos + trueNeg) / (truePos + trueNeg + falsePos + falseNeg)\n",
    "\n",
    "    return accuracy, balancedAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:12:43.487138Z",
     "start_time": "2024-11-28T17:12:30.768072Z"
    }
   },
   "outputs": [],
   "source": [
    "# create deep learning model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def getDLModel(inputShape):\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(inputShape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics = [\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\n",
      "\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.1, Components: 6:\n",
      "Tree accuracy: 0.5555555555555556\n",
      "Tree balanced accuracy: 0.5521978021978022\n",
      "DL accuracy: 0.6666666666666666\n",
      "DL balanced accuracy: 0.7010869565217391\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.2, Components: 13:\n",
      "Tree accuracy: 0.6296296296296297\n",
      "Tree balanced accuracy: 0.6263736263736264\n",
      "DL accuracy: 0.6666666666666666\n",
      "DL balanced accuracy: 0.8200000000000001\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.3, Components: 22:\n",
      "Tree accuracy: 0.5925925925925926\n",
      "Tree balanced accuracy: 0.5961538461538461\n",
      "DL accuracy: 0.5925925925925926\n",
      "DL balanced accuracy: 0.5543478260869565\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.4, Components: 32:\n",
      "Tree accuracy: 0.5925925925925926\n",
      "Tree balanced accuracy: 0.5833333333333333\n",
      "DL accuracy: 0.6666666666666666\n",
      "DL balanced accuracy: 0.8200000000000001\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.5, Components: 43:\n",
      "Tree accuracy: 0.5925925925925926\n",
      "Tree balanced accuracy: 0.5961538461538461\n",
      "DL accuracy: 0.6296296296296297\n",
      "DL balanced accuracy: 0.6458333333333333\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.6, Components: 55:\n",
      "Tree accuracy: 0.6296296296296297\n",
      "Tree balanced accuracy: 0.6164772727272727\n",
      "DL accuracy: 0.5185185185185185\n",
      "DL balanced accuracy: 0.45238095238095233\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.7, Components: 69:\n",
      "Tree accuracy: 0.5555555555555556\n",
      "Tree balanced accuracy: 0.5397727272727273\n",
      "DL accuracy: 0.5555555555555556\n",
      "DL balanced accuracy: 0.28846153846153844\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.8, Components: 85:\n",
      "Tree accuracy: 0.5555555555555556\n",
      "Tree balanced accuracy: 0.5521978021978022\n",
      "DL accuracy: 0.5925925925925926\n",
      "DL balanced accuracy: 0.2962962962962963\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.9, Components: 104:\n",
      "Tree accuracy: 0.4444444444444444\n",
      "Tree balanced accuracy: 0.44780219780219777\n",
      "DL accuracy: 0.5925925925925926\n",
      "DL balanced accuracy: 0.2962962962962963\n",
      "\n",
      "\n",
      "#######################################\n",
      "Max Tree Covariance Floor: 0.2, with balanced accuracy of 62.637362637362635%\n",
      "Max DL Covariance Floor: 0.2, with balanced accuracy of 82.0%\n",
      "#######################################\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#!pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# minimum covariance for feature to be included\n",
    "pcaCovarianceFloor = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "bAccTree = []\n",
    "accTree  = []\n",
    "\n",
    "bAccDL = []\n",
    "accDL  = []\n",
    "\n",
    "numberOfComponents = []\n",
    "\n",
    "for pcaFloor in pcaCovarianceFloor:\n",
    "\n",
    "    # run PCA with given number of components\n",
    "    principal=PCA(n_components=pcaFloor)\n",
    "    principal.fit(Scaled_data)\n",
    "    x=principal.transform(Scaled_data)\n",
    "\n",
    "    # get the reduced dataset\n",
    "    reducedGenderData = principal.transform(Scaled_data)\n",
    "    numberOfComponents.append(reducedGenderData.shape[1])\n",
    "    reducedGenderDF = pd.DataFrame(reducedGenderData, columns=[f\"PC{i}\" for i in range(1, numberOfComponents[-1] + 1)])\n",
    "\n",
    "    # split the dataset into testing and training splits\n",
    "    # since the dataset is so small, we want to make sure \n",
    "    # there is at least 10 of each class in test split\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(reducedGenderDF, y, test_size=0.2, random_state=76)\n",
    "\n",
    "    # train XGBoost model\n",
    "    xgb_classifier = xgb.XGBClassifier()\n",
    "    xgb_classifier.fit(xTrain, yTrain)\n",
    "\n",
    "    # get test accuracy and balanced accuracy\n",
    "    acc, bAcc = getAccAndBAcc(xgb_classifier.predict(xTest), yTest.to_numpy())\n",
    "\n",
    "    # store accuracies\n",
    "    accTree.append(acc)\n",
    "    bAccTree.append(bAcc)\n",
    "\n",
    "    # train DL model\n",
    "    FNN = getDLModel(numberOfComponents[-1])\n",
    "    FNN.fit(xTrain, yTrain, epochs=100, verbose=0)\n",
    "\n",
    "    # get test accuracy and balanced accuracy\n",
    "    acc, bAcc = getAccAndBAcc(FNN.predict(xTest), yTest.to_numpy())\n",
    "\n",
    "    # store accuracies\n",
    "    accDL.append(acc)\n",
    "    bAccDL.append(bAcc)\n",
    "\n",
    "print(\"\\n\")\n",
    "for idx in range(len(pcaCovarianceFloor)):\n",
    "\n",
    "    print(\"#######################################\")\n",
    "    print(f\"Values for Cov. Floor: {pcaCovarianceFloor[idx]}, Components: {numberOfComponents[idx]}:\")\n",
    "    print(f\"Tree accuracy: {accTree[idx]}\")\n",
    "    print(f\"Tree balanced accuracy: {bAccTree[idx]}\")\n",
    "    print(f\"DL accuracy: {accDL[idx]}\")\n",
    "    print(f\"DL balanced accuracy: {bAccDL[idx]}\")\n",
    "\n",
    "\n",
    "maxTreeBAccPCACovFloor = pcaCovarianceFloor[np.argmax(bAccTree)]\n",
    "maxDLBAccPCACovFloor = pcaCovarianceFloor[np.argmax(bAccDL)]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"#######################################\")\n",
    "print(f\"Max Tree Covariance Floor: {maxTreeBAccPCACovFloor}, with balanced accuracy of {np.max(bAccTree) * 100}%\")\n",
    "print(f\"Max DL Covariance Floor: {maxDLBAccPCACovFloor}, with balanced accuracy of {np.max(bAccDL) * 100}%\")\n",
    "print(\"#######################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T18:30:48.541188Z",
     "start_time": "2024-11-28T18:30:47.988573Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Attention, Flatten, Input\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:43:13.714608Z",
     "start_time": "2024-11-28T17:43:13.706322Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_df(input_df, col_to_drop, y_col):\n",
    "    new_df = input_df.drop(col_to_drop, axis=1)\n",
    "    new_df = new_df.dropna()\n",
    "    y_new = new_df.pop(y_col)\n",
    "    tfidf_features = pd.DataFrame(new_df['tfidf_features'].tolist(), index=new_df.index)\n",
    "    tfidf_features.columns = [f'tfidf_features{i}' for i in range(tfidf_features.shape[1])]\n",
    "\n",
    "    count_features = pd.DataFrame(new_df['count_features'].tolist(), index=new_df.index)\n",
    "    count_features.columns = [f'count_features{i}' for i in range(count_features.shape[1])]\n",
    "    \n",
    "    new_df = pd.concat([tfidf_features, count_features], axis=1)\n",
    "    return new_df, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:50:08.139930Z",
     "start_time": "2024-11-28T17:50:07.498883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134, 17404), (134,))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_df, y_race = create_df(combined_data_df, [\"gender\", \"condition\", \"participant_id\", \"PHQ_score\"], \"race\")\n",
    "race_df.shape, y_race.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T18:36:55.424774Z",
     "start_time": "2024-11-28T18:36:55.404106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      0\n",
       "2      3\n",
       "3      0\n",
       "4      2\n",
       "      ..\n",
       "129    4\n",
       "130    2\n",
       "131    0\n",
       "132    2\n",
       "133    2\n",
       "Name: race, Length: 134, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgboost needs the range to start at 0\n",
    "y_race = y_race - 1\n",
    "y_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:49:04.061900Z",
     "start_time": "2024-11-28T17:49:04.044871Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_data(input_df):\n",
    "    scaling=StandardScaler()\n",
    "    scaling.fit(input_df)\n",
    "    Scaled_data=scaling.transform(input_df)\n",
    "    return Scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T18:22:41.261261Z",
     "start_time": "2024-11-28T18:22:41.250044Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dense_model(input_shape):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "    \n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "    \n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "    \n",
    "        Dense(16, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "    \n",
    "        Dense(8, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "    \n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T18:37:29.368230Z",
     "start_time": "2024-11-28T18:37:04.294557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "[2 2 3 2 2 0 2 6 0 6 0 2 0 2 0 2 2 2 2 2 3 2 0 2 2 3 2]\n",
      "[0.58213997 0.9465875  0.9135452  0.9200443  0.97254556 0.88910455\n",
      " 0.93198967 0.98857963 0.9179726  0.9168656  0.95342916 0.89318293\n",
      " 0.02660007 0.91617525 0.95555437 0.97454137 0.5222763  0.9778992\n",
      " 0.983741   0.9231853  0.96315026 0.9467469  0.8984548  0.88574713\n",
      " 0.9868848  0.97462684 0.30373526]\n",
      "[0 2 2 0 0 2 2 0 0 0 0 3 1 2 3 2 0 2 3 3 2 2 2 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "scaled_data = scale_data(race_df)\n",
    "\n",
    "pcaCovarianceFloor = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "bA_dense, ba_xgb = [], []\n",
    "a_dense, a_xgb = [], []\n",
    "\n",
    "num_comp = []\n",
    "\n",
    "for pcaFloor in pcaCovarianceFloor:\n",
    "    \n",
    "    principal=PCA(n_components=pcaFloor)\n",
    "    principal.fit(scaled_data)\n",
    "    x=principal.transform(scaled_data)\n",
    "    \n",
    "    reduced_race_df = principal.transform(scaled_data)\n",
    "    num_comp.append(reduced_race_df.shape[1])\n",
    "    reduced_race_df = pd.DataFrame(reduced_race_df, columns=[f\"PC{i}\" for i in range(1, num_comp[-1] + 1)])\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(reduced_race_df, y_race, test_size=0.2, random_state=76)\n",
    "\n",
    "    attention_model = get_dense_model(num_comp[-1])\n",
    "    attention_model.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "\n",
    "    acc, bAcc = getAccAndBAcc(attention_model.predict(x_test), y_test.to_numpy())\n",
    "\n",
    "    a_dense.append(acc)\n",
    "    bA_dense.append(bAcc)\n",
    "    \n",
    "    xgb_classifier = xgb.XGBClassifier()\n",
    "    xgb_classifier.fit(x_train, y_train)\n",
    "\n",
    "    acc, bAcc = getAccAndBAcc(xgb_classifier.predict(x_test), y_test.to_numpy())\n",
    "\n",
    "    \n",
    "    a_xgb.append(acc)\n",
    "    ba_xgb.append(bAcc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T18:43:44.315866Z",
     "start_time": "2024-11-28T18:43:44.284826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.1, Components: 6:\n",
      "Tree accuracy: 0.8888888888888888\n",
      "Tree balanced accuracy: 0.4444444444444444\n",
      "DL accuracy: 0.8888888888888888\n",
      "DL balanced accuracy: 0.4444444444444444\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.2, Components: 13:\n",
      "Tree accuracy: 0.8888888888888888\n",
      "Tree balanced accuracy: 0.4444444444444444\n",
      "DL accuracy: 0.8888888888888888\n",
      "DL balanced accuracy: 0.4444444444444444\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.3, Components: 22:\n",
      "Tree accuracy: 0.8888888888888888\n",
      "Tree balanced accuracy: 0.4444444444444444\n",
      "DL accuracy: 0.8888888888888888\n",
      "DL balanced accuracy: 0.4444444444444444\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.4, Components: 32:\n",
      "Tree accuracy: 0.8888888888888888\n",
      "Tree balanced accuracy: 0.4444444444444444\n",
      "DL accuracy: 0.8888888888888888\n",
      "DL balanced accuracy: 0.4444444444444444\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.5, Components: 44:\n",
      "Tree accuracy: 0.8888888888888888\n",
      "Tree balanced accuracy: 0.4444444444444444\n",
      "DL accuracy: 0.8888888888888888\n",
      "DL balanced accuracy: 0.4444444444444444\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.6, Components: 56:\n",
      "Tree accuracy: 0.8888888888888888\n",
      "Tree balanced accuracy: 0.4444444444444444\n",
      "DL accuracy: 0.8888888888888888\n",
      "DL balanced accuracy: 0.4444444444444444\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.7, Components: 70:\n",
      "Tree accuracy: 0.8888888888888888\n",
      "Tree balanced accuracy: 0.4444444444444444\n",
      "DL accuracy: 0.8888888888888888\n",
      "DL balanced accuracy: 0.4444444444444444\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.8, Components: 87:\n",
      "Tree accuracy: 0.8888888888888888\n",
      "Tree balanced accuracy: 0.4444444444444444\n",
      "DL accuracy: 0.8888888888888888\n",
      "DL balanced accuracy: 0.4444444444444444\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.9, Components: 106:\n",
      "Tree accuracy: 0.8888888888888888\n",
      "Tree balanced accuracy: 0.4444444444444444\n",
      "DL accuracy: 0.8888888888888888\n",
      "DL balanced accuracy: 0.4444444444444444\n",
      "\n",
      "\n",
      "#######################################\n",
      "Max Tree Covariance Floor: 0.1, with balanced accuracy of 44.44444444444444%\n",
      "Max DL Covariance Floor: 0.1, with balanced accuracy of 44.44444444444444%\n",
      "#######################################\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "for idx in range(len(pcaCovarianceFloor)):\n",
    "\n",
    "    print(\"#######################################\")\n",
    "    print(f\"Values for Cov. Floor: {pcaCovarianceFloor[idx]}, Components: {num_comp[idx]}:\")\n",
    "    print(f\"Tree accuracy: {a_xgb[idx]}\")\n",
    "    print(f\"Tree balanced accuracy: {ba_xgb[idx]}\")\n",
    "    print(f\"DL accuracy: {a_dense[idx]}\")\n",
    "    print(f\"DL balanced accuracy: {bA_dense[idx]}\")\n",
    "\n",
    "\n",
    "maxTreeBAccPCACovFloor = pcaCovarianceFloor[np.argmax(ba_xgb)]\n",
    "maxDLBAccPCACovFloor = pcaCovarianceFloor[np.argmax(bA_dense)]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"#######################################\")\n",
    "print(f\"Max Tree Covariance Floor: {maxTreeBAccPCACovFloor}, with balanced accuracy of {np.max(ba_xgb) * 100}%\")\n",
    "print(f\"Max DL Covariance Floor: {maxDLBAccPCACovFloor}, with balanced accuracy of {np.max(bA_dense) * 100}%\")\n",
    "print(\"#######################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absRelErr(yPred, yTrue):\n",
    "    return np.mean(np.abs(yPred - yTrue) / np.max(yTrue))\n",
    "\n",
    "\n",
    "\n",
    "def pearsonCorr(yPred, yTrue):\n",
    "    return np.corrcoef(yPred, yTrue)[0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getDLModelPHQ(inputShape):\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(inputShape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics = [\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "DL: [13.337463   2.258594   4.4206686  4.718963  12.492231   5.1419744\n",
      " 12.062782  12.091892   3.3506124  5.1999702  3.191227   5.011941\n",
      " 13.5868845  4.8835373  9.297262  12.555527  15.228771   4.727675\n",
      " 13.370886   2.4170954  2.0760539  3.0328467  4.8470025  3.6417987\n",
      "  4.6540422  6.291273  16.052652 ]\n",
      "XGB: [ 5.090878   4.273607   4.4827123  4.713615  12.078203  12.830768\n",
      "  5.066999  12.960701   6.7639174  4.727186   7.1131506  7.7622232\n",
      "  7.646736   6.0238395  9.4570875  7.1776958  4.96517    6.283907\n",
      "  5.917521   4.4232993  1.436622   6.660711   3.411708   6.949372\n",
      "  9.9961195  4.734971   4.9377565]\n",
      "True: [ 1 10  0  1  6  1  1  8  2  2  0  9  0  0  3 15  2  1  9 16  4  0  4  0\n",
      "  3  1  2]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    }
   ],
   "source": [
    "phq_df, y_phq = create_df(combined_data_df, [\"gender\", \"condition\", \"participant_id\", \"race\"], \"PHQ_score\")\n",
    "\n",
    "scaled_data = scale_data(phq_df)\n",
    "\n",
    "pcaCovarianceFloor = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "pcaCovarianceFloor = [0.5]\n",
    "\n",
    "re_dense, re_xgb = [], []\n",
    "r_dense, r_xgb = [], []\n",
    "\n",
    "num_comp = []\n",
    "\n",
    "for pcaFloor in pcaCovarianceFloor:\n",
    "\n",
    "    principal=PCA(n_components=pcaFloor)\n",
    "    principal.fit(scaled_data)\n",
    "    x=principal.transform(scaled_data)\n",
    "    \n",
    "    reduced_phq_df = principal.transform(scaled_data)\n",
    "    num_comp.append(reduced_phq_df.shape[1])\n",
    "    reduced_phq_df = pd.DataFrame(reduced_phq_df, columns=[f\"PC{i}\" for i in range(1, num_comp[-1] + 1)])\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(reduced_phq_df, y_phq, test_size=0.2, random_state=76)\n",
    "\n",
    "    attention_model = getDLModelPHQ(num_comp[-1])\n",
    "    attention_model.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "\n",
    "    relErr = absRelErr(attention_model.predict(x_test), y_test.to_numpy())\n",
    "    re_dense.append(relErr)\n",
    "    \n",
    "    xgb_regressor = xgb.XGBRegressor()\n",
    "    xgb_regressor.fit(x_train, y_train)\n",
    "\n",
    "    relErr = absRelErr(xgb_regressor.predict(x_test), y_test.to_numpy())\n",
    "    re_xgb.append(relErr)\n",
    "\n",
    "    pearson = pearsonCorr(xgb_regressor.predict(x_test), y_test.to_numpy())\n",
    "    r_xgb.append(pearson)\n",
    "\n",
    "    pearson = pearsonCorr(attention_model.predict(x_test).flatten(), y_test.to_numpy())\n",
    "    r_dense.append(pearson)\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.1, Components: 6:\n",
      "Tree Abs. Rel. Err.: 0.33117100072127803\n",
      "Tree Pearson Corr.: -0.10339518775899634\n",
      "DL Abs. Rel. Err.: 0.25367001057774935\n",
      "DL Pearson Corr.: 0.183301031282902\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.2, Components: 13:\n",
      "Tree Abs. Rel. Err.: 0.2908648470485652\n",
      "Tree Pearson Corr.: 0.02956304302284035\n",
      "DL Abs. Rel. Err.: 0.9675216216527549\n",
      "DL Pearson Corr.: -0.005082451557327767\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.3, Components: 22:\n",
      "Tree Abs. Rel. Err.: 0.30529405176639557\n",
      "Tree Pearson Corr.: -0.15847892585774917\n",
      "DL Abs. Rel. Err.: 0.6489768052276924\n",
      "DL Pearson Corr.: -0.0467748554974345\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.4, Components: 32:\n",
      "Tree Abs. Rel. Err.: 0.29765791901283795\n",
      "Tree Pearson Corr.: 0.13526043382162525\n",
      "DL Abs. Rel. Err.: 0.5747569360769007\n",
      "DL Pearson Corr.: -0.023756786148974054\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.5, Components: 44:\n",
      "Tree Abs. Rel. Err.: 0.32889149006870055\n",
      "Tree Pearson Corr.: 0.043021939419424324\n",
      "DL Abs. Rel. Err.: 0.3926525116171202\n",
      "DL Pearson Corr.: 0.0761741702886662\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.6, Components: 56:\n",
      "Tree Abs. Rel. Err.: 0.2679312902468222\n",
      "Tree Pearson Corr.: 0.015462457034222167\n",
      "DL Abs. Rel. Err.: 0.498197925730579\n",
      "DL Pearson Corr.: -0.08427439304412608\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.7, Components: 70:\n",
      "Tree Abs. Rel. Err.: 0.287816298228723\n",
      "Tree Pearson Corr.: 0.06686048754079627\n",
      "DL Abs. Rel. Err.: 0.3836785224464368\n",
      "DL Pearson Corr.: 0.04322482483673493\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.8, Components: 87:\n",
      "Tree Abs. Rel. Err.: 0.31551165050930446\n",
      "Tree Pearson Corr.: -0.1080726387771086\n",
      "DL Abs. Rel. Err.: 0.5990634436408678\n",
      "DL Pearson Corr.: -0.13369280514618373\n",
      "#######################################\n",
      "Values for Cov. Floor: 0.9, Components: 106:\n",
      "Tree Abs. Rel. Err.: 0.3336053788661957\n",
      "Tree Pearson Corr.: -0.15991408637519422\n",
      "DL Abs. Rel. Err.: 0.3967190569545179\n",
      "DL Pearson Corr.: 0.13456342889487025\n",
      "\n",
      "\n",
      "#######################################\n",
      "Min Tree Covariance Floor: 0.6, with absolute relative error of 0.2679312902468222\n",
      "Min DL Covariance Floor: 0.1, with absolute relative error of 0.25367001057774935\n",
      "#######################################\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "\n",
    "for idx in range(len(pcaCovarianceFloor)):\n",
    "    print(\"#######################################\")\n",
    "    print(f\"Values for Cov. Floor: {pcaCovarianceFloor[idx]}, Components: {num_comp[idx]}:\")\n",
    "    print(f\"Tree Abs. Rel. Err.: {re_xgb[idx]}\")\n",
    "    print(f\"Tree Pearson Corr.: {r_xgb[idx]}\")\n",
    "    print(f\"DL Abs. Rel. Err.: {re_dense[idx]}\")\n",
    "    print(f\"DL Pearson Corr.: {r_dense[idx]}\")\n",
    "\n",
    "maxTreeREPCACovFloor = pcaCovarianceFloor[np.argmin(re_xgb)]\n",
    "maxDLREPCACovFloor = pcaCovarianceFloor[np.argmin(re_dense)]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"#######################################\")\n",
    "print(f\"Min Tree Covariance Floor: {maxTreeREPCACovFloor}, with absolute relative error of {np.min(re_xgb)}\")\n",
    "print(f\"Min DL Covariance Floor: {maxDLREPCACovFloor}, with absolute relative error of {np.min(re_dense)}\")\n",
    "print(\"#######################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
